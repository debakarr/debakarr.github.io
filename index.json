[{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"This is going to be a walkthrough of one of the LAB I did in Udemy to build Serverless Workflow in AWS. The task of the LAB was to take any JSON file uploaded in S3 bucket and store the data from that in DynamoDB. Thing it covers: Efficiently process AWS S3 events using AWS SQS Message Queues. Trigger AWS Lambda Function to process the message in SQS queue. We can process the file in AWS Lambda Function if required. Store the data to AWS DynamoDB. This is written throught the AWS Lambda Funtion. IAM Role can be used to manage access. ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:0:0","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Creating S3 bucket for storing files Navigate to console.aws.amazon.com/console/home Click “Services” Click “Storage” Click “S3” Click “Create bucket” Give some unique name for bucket like “json-processing-bucket” Click “Create bucket” ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:1:0","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Creating Table in DynamoDB Navigate to console.aws.amazon.com/console/home Click “Services” Click “Database” Click “DynamoDB” Click “Create table” Give some unique name for DB Table like “JSONItemTable” and provide a primary key like “id”. This will be later used to update data via the AWS Lambda function. Click “Create table” ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:2:0","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Create Lambda Function ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:3:0","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Create role for Lambda Function Before creating Lambda Function we can set role and limit access to what this lambda funtion can do. Basically we want this Lambda Function to: Have read access to S3 bucket -\u003e This is possible via AmazonS3ReadOnlyAccess Receive message form SQS -\u003e This is possible via AWSLambdaSQSQueueExecutionRole Have write access to Dynamo DB -\u003e AmazonDynamoDBFullAccess Have write access to CloudWatch in case we want to debug -\u003e AWSLambdaBasicExecutionRole To create this role Click “Services” Click “Security, Identity, \u0026 Compliance” Click “IAM” Click “Roles” Click “Create role” Select “AWS Service” and the “Lambda” Click “Next” Filter the roles. You can filter with this “AmazonS3ReadOnlyAccess|AWSLambdaSQSQueueExecutionRole|AmazonDynamoDBFullAccess|AWSLambdaBasicExecutionRole” Click “Next” Provide a name to the role like “LambdaRoleForJSONItems” Click “Create role” ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:3:1","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Create Lambda Function Click “Services” Click “Compute” Click “Lambda” Click “Create function” Click “Author from scrach” Provide a Function name. Here I am calling it “JSONProcessingLambdaFunctionTriggeredBySQS”. Also I am going to use Python as Runtime. Now we can use the role which we created and link it with this Lambda Function. Click on “Create function” Next screen we can add the Lambda “Code”. I am going to add the Python Code. It is well documented. Please open the code block below to view the complete code. import json import logging import random import boto3 sqs_client = boto3.client('sqs') dynamo_client = boto3.resource('dynamodb') s3_client = boto3.client('s3') # Set up logger logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): # Log SQS event logger.info({ 'message': 'Received SQS event', 'event': event, 'awsRequestId': context.aws_request_id, 'functionName': context.function_name }) # Extract S3 event from SQS message body s3_event = json.loads(event['Records'][0]['body']) # Log S3 event logger.info({ 'message': 'Received S3 event', 'event': s3_event, 'awsRequestId': context.aws_request_id, 'functionName': context.function_name }) # Extract source bucket and key from S3 event src_bucket = s3_event['Records'][0]['s3']['bucket']['name'] src_key = s3_event['Records'][0]['s3']['object']['key'] # Set parameters for getting S3 object s3_params = { 'Bucket': src_bucket, 'Key': src_key } # Get S3 object and read its content as string s3_obj = s3_client.get_object(**s3_params) s3_body = s3_obj['Body'].read().decode('utf-8') # Log S3 object body logger.info({ 'message': 'Retrieved S3 object body', 's3ObjectBody': s3_body, 'awsRequestId': context.aws_request_id, 'functionName': context.function_name }) # Parse S3 object body as JSON item = json.loads(s3_body) # Generate a random ID and add it to the JSON item item['id'] = str(random.random() * (10**16)) # Get DynamoDB table for processed items processed_items_table = dynamo_client.Table('JSONItemTable') # Put the JSON item into the DynamoDB table result = processed_items_table.put_item(Item=item) # Log result logger.info({ 'message': 'Put item into DynamoDB table', 'result': result, 'awsRequestId': context.aws_request_id, 'functionName': context.function_name }) return result Click “Deploy” ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:3:2","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Application integration Here we would like to a queue which will take the input from S3 bucket and pass it to our Lambda function. To have high resilience we will also have a dead-letter queue which will be containing all the json which are not processed properly. We will have a retention of 14 days for those json items. ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:4:0","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Creating dead-letter queue Click “Services” Click “Application Integration” Click “Simple Queue Service” Click “Create queue” Provide a name to dead-letter queue. Here I am giving it a name “JSONDeadLetterQueue” We can also set the retention policy. For now I am setting it to 14 days. Click “Create queue”. ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:4:1","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Create queue for S3 bucket event notification Click back “Queue” Click “Create queue” again. Set the name for the queue. I am naming it “JSONProcessingQueue”. Next step will be to modify the access policy for the queue. For this you will need the account ID as well as the S3 bucket name. You can get the account ID under the profile name. Under “Access policy” click on “Advanced” and paste the following json. Replace with yours\rPlease note that you will have to modify it as per your account name and S3 bucket name.\r{ \"Version\": \"2012-10-17\", \"Id\": \"JSONProcessingQueue-ID\", \"Statement\": [ { \"Sid\": \"JSONProcessingQueue-statement-ID\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"SQS:SendMessage\", \"Resource\": \"arn:aws:sqs:us-east-1:CHANGE-WITH-YOUR-ACCOUNT-ID:JSONProcessingQueue\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"CHANGE-WITH-YOUR-ACCOUNT-ID\" }, \"ArnLike\": { \"aws:SourceArn\": \"arn:aws:s3:*:*:CHANGE-WITH-YOUR-BUCKET-NAME\" } } } ] } Select the dead-letter queue we had created in the previous section. Click “Create queue” Click “Lambda triggers” Click “Configure Lambda function trigger” Select the lambda function which we created before. Click “Save” ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:4:2","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Push event notification from S3 to SQS Click “Services” Click “Storage” Click “S3” Click “json-processing-bucket” (Your bucket name might be different) Click “Properties” Click “Create event notification” Give the event notification a name. I have give name as “sqs-event-notification”. Also I have given access to All object creation. So anytime a new object is pushed to S3, it will trigger a event to the SQS. We can link the SQS under “Destination” Click “Save changes” ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:5:0","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"content":"Try out to see if the DynamoDB database is getting updated on new push Click “Services” Click “Storage” Click “S3” Click “json-processing-bucket” Click “Upload” Click “Add files” Click “Upload” and select the file. For demostration purpose try to upload a valid JSON file Click “Services” Click “Database” Click “DynamoDB” Click “Tables” Click “JSONItemTable” Click “Explore table items” Verify that the data is uploaded properly That covers the whole process of deploying a simple serverless application using AWS lambda. ","date":"2023-05-13","objectID":"/posts/aws/s3todb/:6:0","tags":["DevOps","AWS","S3","SQS","Serverless","Lambda"],"title":"Serverless Workflow to Process Files Uploaded to Amazon S3","uri":"/posts/aws/s3todb/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","IaC"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbook Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Infrastructure as Code (IaC) is a methodology that involves managing and provisioning infrastructure using code instead of manual processes. It allows developers and system administrators to automate the deployment and management of infrastructure, reducing the potential for human error and increasing efficiency. Here are some key points to understand about IaC: 🔧 Tools and Languages: IaC can be implemented using various tools and languages such as Terraform, Ansible, Puppet, Chef, CloudFormation, YAML, JSON, and more. These tools provide a way to define, provision, and manage infrastructure as code. 🧩 Declarative vs Imperative: IaC can be categorized into two types - declarative and imperative. Declarative IaC focuses on defining the desired end state of the infrastructure and letting the tool handle the implementation details. Imperative IaC, on the other hand, involves specifying the exact steps and commands needed to create the infrastructure. 📦 Infrastructure Components: IaC can be used to manage various infrastructure components such as servers, databases, load balancers, networks, and more. With IaC, these components can be easily replicated and scaled up or down depending on the needs of the org","date":"2023-05-09","objectID":"/posts/devops/day21-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","IaC","Terraform"],"title":"Day 21: Introduction to Infrastructure as Code (IaC)","uri":"/posts/devops/day21-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbook Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Configuration management tools such as Puppet and Chef have become increasingly important in managing large-scale IT infrastructures. In this post, we will provide an overview of Puppet and Chef and compare their features and capabilities. ","date":"2023-05-08","objectID":"/posts/devops/day20-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Chef","Puppet","Comparision"],"title":"Day 20: Puppet and Chef - Overview and comparison","uri":"/posts/devops/day20-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"🎭 Puppet 👉 Puppet is an open-source configuration management tool used for automating the deployment and management of software and system configurations. Puppet uses a declarative language, called Puppet DSL, to define and manage system configurations. This language allows users to specify the desired system state without having to write detailed scripts. Some of the key features of Puppet include: Idempotency: Puppet ensures that the desired system state is maintained, even in the face of unexpected changes or errors. Agent-based architecture: Puppet uses a client-server architecture, where the agent runs on each node and communicates with the Puppet master server to receive configuration updates. Resource abstraction: Puppet abstracts system resources, such as files, services, and packages, into resource types, which can be easily managed through Puppet DSL. ","date":"2023-05-08","objectID":"/posts/devops/day20-devops/:1:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Chef","Puppet","Comparision"],"title":"Day 20: Puppet and Chef - Overview and comparison","uri":"/posts/devops/day20-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"🍴 Chef 👉 Chef is another popular open-source configuration management tool used for automating IT infrastructure. Chef uses a Ruby-based DSL to define and manage system configurations. This language allows users to write detailed scripts that define the desired system state. Some of the key features of Chef include: Flexibility: Chef allows users to define and customize their own cookbooks and recipes, which can be easily shared and reused. Test-driven development: Chef supports test-driven development practices through the use of test-kitchen and other testing tools. Container support: Chef has strong support for containers and microservices, which makes it a good choice for managing modern IT environments. ","date":"2023-05-08","objectID":"/posts/devops/day20-devops/:2:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Chef","Puppet","Comparision"],"title":"Day 20: Puppet and Chef - Overview and comparison","uri":"/posts/devops/day20-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"🔍 Comparison Both Puppet and Chef have similar goals and features, but they differ in their approach and implementation. Here are some key differences between Puppet and Chef: Puppet uses a declarative language, called Puppet DSL, while Chef uses a Ruby-based DSL. Puppet has stronger support for Windows and cloud platforms, while Chef has stronger support for containers and microservices. Puppet has a larger and more established community, while Chef has a more flexible and customizable architecture. In conclusion, both Puppet and Chef are powerful configuration management tools that can help automate and streamline IT infrastructure management. The choice between them depends on the specific needs and requirements of your organization. There is a very nice comprehensive video going through some of the most used configuration management tool Simplilearn have other videos also discussing all of these tools in more details. ","date":"2023-05-08","objectID":"/posts/devops/day20-devops/:3:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Chef","Puppet","Comparision"],"title":"Day 20: Puppet and Chef - Overview and comparison","uri":"/posts/devops/day20-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible","Best Pactice"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbook Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance 👋 In this blog post, we will explore Ansible roles, their benefits, and how to use them in your playbooks. We’ll also discuss best practices for organizing and structuring your Ansible projects. ","date":"2023-05-07","objectID":"/posts/devops/day19-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Ansible-Galaxy","Ansible Roles","Best Pactice"],"title":"Day 19: Ansible - Roles and best practices","uri":"/posts/devops/day19-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible","Best Pactice"],"content":"Ansible Roles: Basics and Creating Ansible roles provide a well-defined structure for organizing your tasks, variables, handlers, metadata, templates, and other files. They allow you to reuse and share your Ansible code efficiently across multiple projects without duplicating the code. This makes your projects more manageable and easier to maintain. A simple example of using roles in a playbook can be found in the official Ansible documentation: --- - hosts: webservers roles: - common - webservers In this example, the common and webservers roles are applied to the webservers group of hosts. ","date":"2023-05-07","objectID":"/posts/devops/day19-devops/:0:1","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Ansible-Galaxy","Ansible Roles","Best Pactice"],"title":"Day 19: Ansible - Roles and best practices","uri":"/posts/devops/day19-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible","Best Pactice"],"content":"Sharing Roles with Ansible Galaxy Ansible Galaxy is a public repository for sharing and discovering Ansible roles created by the community. You can search and download roles that fit your use case, saving time and leveraging the expertise of other users. For example, to install a PostgreSQL server role from Ansible Galaxy, you can run: $ ansible-galaxy install geerlingguy.postgresql Starting galaxy role install process - downloading role 'postgresql', owned by geerlingguy - downloading role from https://github.com/geerlingguy/ansible-role-postgresql/archive/3.4.3.tar.gz - extracting geerlingguy.postgresql to /home/debakarr/.ansible/roles/geerlingguy.postgresql - geerlingguy.postgresql (3.4.3) was installed successfully And then use it in a playbook: --- - hosts: linux-host become: true roles: - role: geerlingguy.postgresql vars: postgresql_users: - name: debakarr For best practices you can follow these resources: Ansible Best Practices Ansible Best practices Ansible Best Practices to Follow [Tips \u0026 Tricks] ","date":"2023-05-07","objectID":"/posts/devops/day19-devops/:0:2","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Ansible-Galaxy","Ansible Roles","Best Pactice"],"title":"Day 19: Ansible - Roles and best practices","uri":"/posts/devops/day19-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbook Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance ","date":"2023-05-06","objectID":"/posts/devops/day18-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Playbook","Ad-hoc command"],"title":"Day18: Ansible - Ad-hoc commands and playbooks","uri":"/posts/devops/day18-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Introduction: Ansible is a powerful automation tool that simplifies complex tasks in IT environments. In this blog post, we will explore Ansible ad-hoc commands and playbooks. Ad-hoc commands are a quick way to execute simple tasks on remote hosts, while playbooks allow you to create reusable and more complex automation workflows. ","date":"2023-05-06","objectID":"/posts/devops/day18-devops/:1:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Playbook","Ad-hoc command"],"title":"Day18: Ansible - Ad-hoc commands and playbooks","uri":"/posts/devops/day18-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Setup I had setup 1 Ubuntu ansible controller and 3 Debian ansible host machine. ","date":"2023-05-06","objectID":"/posts/devops/day18-devops/:2:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Playbook","Ad-hoc command"],"title":"Day18: Ansible - Ad-hoc commands and playbooks","uri":"/posts/devops/day18-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Ansible Ad-hoc commands Ad-hoc commands are a great way to perform quick tasks on your remote hosts without the need for a playbook. These commands are executed directly from the command line and are useful for simple tasks like checking the status of a service or creating a new user. Here are some examples of Ansible ad-hoc commands: Check if linux host are reachable ansible linux-host -m ping Check uptime of all the linux host ansible linux-host -m command -a \"uptime\" Use ansible/builtin.shell module ansible linux-host -m ansible.builtin.shell -a 'echo $TERM' Use ansible.builtin.copy module ansible linux-host -m ansible.builtin.copy -a \"src=/etc/hosts dest=/tmp/hosts\" Keep in mind that ad-hoc commands are not suitable for complex tasks, and their usage should be limited to simple and quick tasks. ","date":"2023-05-06","objectID":"/posts/devops/day18-devops/:2:1","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Playbook","Ad-hoc command"],"title":"Day18: Ansible - Ad-hoc commands and playbooks","uri":"/posts/devops/day18-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Ansible Playbook Playbooks are the core of Ansible’s automation capabilities. They are written in YAML and define a series of tasks to be executed on remote hosts. Playbooks can be easily shared, reused, and versioned, making them ideal for more complex tasks and workflows. Here’s a simple example of an Ansible playbook that installs and configures the Apache web server on multiple Debian hosts under the “linux-host” group: --- - name: Install and configure Apache on Debian hosts hosts: linux-host become: yes tasks: - name: Update package index apt: update_cache: yes - name: Install Apache apt: name: apache2 state: present - name: Start and enable Apache service service: name: apache2 state: started enabled: yes handlers: - name: Restart Apache service: name: apache2 state: restarted In this playbook, we first update the package index on all hosts in the “linux-host” group. Then, we install the Apache web server using the apt module. Finally, we start and enable the Apache service, and create a handler to restart the service if any changes are made. To run this playbook, save it as apache_setup.yml and execute the following command: ansible-playbook apache_setup.yml This playbook will install and configure the Apache web server on all Debian hosts in the “linux-host” group. ","date":"2023-05-06","objectID":"/posts/devops/day18-devops/:2:2","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Playbook","Ad-hoc command"],"title":"Day18: Ansible - Ad-hoc commands and playbooks","uri":"/posts/devops/day18-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Conclusion In this blog post, we have explored Ansible ad-hoc commands and playbooks. Ad-hoc commands are useful for quick and simple tasks, while playbooks provide a more powerful and reusable way to automate complex tasks and workflows. As you continue to learn and use Ansible, you’ll find that these tools can greatly simplify your IT automation tasks and improve your overall workflow. Few more resources one can read: Introduction to ad hoc commands CI/CD concepts How to install software packages with an Ansible playbook ","date":"2023-05-06","objectID":"/posts/devops/day18-devops/:2:3","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible","Playbook","Ad-hoc command"],"title":"Day18: Ansible - Ad-hoc commands and playbooks","uri":"/posts/devops/day18-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Ansible can only be installed in Linux system. Although we can connect Windows machine in Ansible inventory. ","date":"2023-05-05","objectID":"/posts/devops/day17-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible"],"title":"Day 17: Ansible - Installation and configuration","uri":"/posts/devops/day17-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Update System Debian-Based System (Ubuntu, Debian) sudo apt-get update sudo apt-get-upgrade RHEL-Based system (CentOS, RGEL, Fedora)\" If you are using RHEL-Based system then the command will be sudo yum update ","date":"2023-05-05","objectID":"/posts/devops/day17-devops/:0:1","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible"],"title":"Day 17: Ansible - Installation and configuration","uri":"/posts/devops/day17-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Install Python Debian-Based System (Ubuntu, Debian) sudo apt-get install python3 RHEL-Based system (CentOS, RGEL, Fedora)\" If you are using RHEL-Based system then the command will be sudo yum install python3 ","date":"2023-05-05","objectID":"/posts/devops/day17-devops/:0:2","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible"],"title":"Day 17: Ansible - Installation and configuration","uri":"/posts/devops/day17-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Install Ansible Debian-Based System (Ubuntu, Debian) sudo apt-get install ansible RHEL-Based system (CentOS, RGEL, Fedora)\" If you are using RHEL-Based system then the command will be sudo yum install ansible ","date":"2023-05-05","objectID":"/posts/devops/day17-devops/:0:3","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible"],"title":"Day 17: Ansible - Installation and configuration","uri":"/posts/devops/day17-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Verify the version ansible --version ","date":"2023-05-05","objectID":"/posts/devops/day17-devops/:0:4","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible"],"title":"Day 17: Ansible - Installation and configuration","uri":"/posts/devops/day17-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Ansible"],"content":"Configure Ansible Generally we will be modifying /etc/ansible/ansible.cfg file change/update any Ansible configuration. We can also add inventory file in /etc/ansible location. To start with we can add a file name hosts in the /etc/ansible folder. Content of the file may look something like this: [webserver] 192.168.1.0 196.168.1.11 And then update the /etc/ansible/ansible.cfg file: inventory = /etc/ansible/hosts Now ansible is installed and ready to use. ","date":"2023-05-05","objectID":"/posts/devops/day17-devops/:0:5","tags":["DevOps","CI/CD","Jenkins","Tutorial","Ansible"],"title":"Day 17: Ansible - Installation and configuration","uri":"/posts/devops/day17-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Now it’s time to check configuration management. ","date":"2023-05-04","objectID":"/posts/devops/day16-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 16: Introduction to configuration management","uri":"/posts/devops/day16-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"What is Configuration Management? Configuration management is the process of systematically managing, organizing, and maintaining the configuration of computer systems, servers, and software applications. It ensures that the systems and applications are set up and run consistently across different environments. Configuration management tools help automate the deployment, configuration, and management of systems and applications, reducing the risk of human error and ensuring a consistent and reliable environment. ","date":"2023-05-04","objectID":"/posts/devops/day16-devops/:1:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 16: Introduction to configuration management","uri":"/posts/devops/day16-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Why is Configuration Management Important? Configuration management brings numerous benefits to the table: Consistency: Ensuring that your systems are consistently configured helps reduce errors and downtime, leading to a more stable environment. Efficiency: Automating configuration management tasks reduces manual labor, freeing up your team to focus on other important tasks. Scalability: Configuration management makes it easier to deploy and manage new systems, allowing your infrastructure to grow with your business. Compliance: Properly managing configurations can help you meet regulatory and security requirements, protecting your organization from potential legal and financial risks. ","date":"2023-05-04","objectID":"/posts/devops/day16-devops/:2:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 16: Introduction to configuration management","uri":"/posts/devops/day16-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Tooling We can take a look into some of the popular tool in the configuration management space and maybe take an example of say installing and starting an Apache server using these tools. ","date":"2023-05-04","objectID":"/posts/devops/day16-devops/:3:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 16: Introduction to configuration management","uri":"/posts/devops/day16-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Ansible An open-source automation tool that uses a simple, human-readable language called YAML to define automation tasks. Ansible uses an agentless architecture, which means it can manage remote systems without installing any software on them. Example of an Ansible playbook: --- - name: Install and configure Apache hosts: webservers tasks: - name: Install Apache apt: name: apache2 state: present - name: Start and enable Apache service: name: apache2 state: started enabled: yes ","date":"2023-05-04","objectID":"/posts/devops/day16-devops/:3:1","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 16: Introduction to configuration management","uri":"/posts/devops/day16-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Puppet An open-source configuration management tool that uses a declarative language to describe the desired state of a system. Puppet uses an agent-master architecture, where agents run on the managed systems and communicate with a central Puppet master server. Example of a Puppet manifest: package { 'apache2': ensure =\u003e installed, } service { 'apache2': ensure =\u003e running, enable =\u003e true, } ","date":"2023-05-04","objectID":"/posts/devops/day16-devops/:3:2","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 16: Introduction to configuration management","uri":"/posts/devops/day16-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Chef An open-source configuration management tool that uses Ruby-based Domain Specific Language (DSL) to define the desired state of a system. Chef uses a client-server architecture, with a central Chef server managing the distribution of configuration data to managed nodes. Example of a Chef recipe: package 'apache2' do action :install end service 'apache2' do action [:enable, :start] end ","date":"2023-05-04","objectID":"/posts/devops/day16-devops/:3:3","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 16: Introduction to configuration management","uri":"/posts/devops/day16-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Some good resource for best practice in Jenkins: New Best Practices for Jenkins Pipeline Global Shared Libraries Pipeline Best Practices Top 10 Best Practices for Jenkins Pipeline Plugin ","date":"2023-05-03","objectID":"/posts/devops/day15-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Best Practices"],"title":"Day 15: Jenkins - Pipelines and best practices","uri":"/posts/devops/day15-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Again CloudBeesTV have some nice resources which I used to follow: One essential aspect everyone should be aware of is that, at times, when managing multiple projects, you might notice that each project contains a Jenkinsfile. If you have projects of the same type, such as Python projects, the Jenkinsfiles in all of them may appear identical, with only minor parameter differences. Due to this reason, we utilize Jenkins Shared Libraries. ","date":"2023-05-02","objectID":"/posts/devops/day14-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Git","GitHub","Jenkins Pipeline","Jenkins Credential","Multibranch Pipeline","Shared Libraries"],"title":"Day 14: Jenkins - Integrating with ","uri":"/posts/devops/day14-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Again CloudBeesTV have some nice resources which I used to follow: ","date":"2023-05-01","objectID":"/posts/devops/day13-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","Freestyle Job","Pipeline Job","Jenkins Build","Jenkins Job"],"title":"Day 13: Jenkins - Creating and managing jobs","uri":"/posts/devops/day13-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Best resource to refer while installing and configuring Jenkins is the official guide. But there are several video Tutorial especially from CloudBee Jenkins’s Youtube channel which would be my go to place to look for how to install and configure Jenkins. ","date":"2023-04-29","objectID":"/posts/devops/day12-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial"],"title":"Day 12: Jenkins - Installation and configuration","uri":"/posts/devops/day12-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"How to install Jenkins on Ubuntu ","date":"2023-04-29","objectID":"/posts/devops/day12-devops/:0:1","tags":["DevOps","CI/CD","Jenkins","Tutorial"],"title":"Day 12: Jenkins - Installation and configuration","uri":"/posts/devops/day12-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"How to install Jenkins on Windows ","date":"2023-04-29","objectID":"/posts/devops/day12-devops/:0:2","tags":["DevOps","CI/CD","Jenkins","Tutorial"],"title":"Day 12: Jenkins - Installation and configuration","uri":"/posts/devops/day12-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"How to install Jenkins on Mac There are other tutorial as well the the channel on How to Install Jenkins in X. ","date":"2023-04-29","objectID":"/posts/devops/day12-devops/:0:3","tags":["DevOps","CI/CD","Jenkins","Tutorial"],"title":"Day 12: Jenkins - Installation and configuration","uri":"/posts/devops/day12-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance CI/CD is all about streamlining your software development process. This is mostly discussed in Day 1 post. ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:0:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Highlight Continuous Integration (CI): Involves merging code changes from multiple developers into a shared repository (e.g., Git) as frequently as possible. Automated builds and tests are run on each integration to ensure that the merged code is free of errors and conflicts. Continuous Deployment (CD): Involves automating the process of deploying the tested and verified code to production environments. This ensures that new features and bug fixes are delivered to users quickly and efficiently. ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:1:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Benifits of CI/CD Faster feedback: Automated tests and builds provide immediate feedback on code changes, allowing developers to identify and fix issues early in the development process. Improved collaboration: CI/CD practices encourage frequent code integrations, which help reduce the risk of conflicts and improve collaboration among team members. Higher code quality: Regularly running automated tests ensures that code quality is maintained and that new changes do not introduce regressions. Faster delivery: By automating the deployment process, new features and bug fixes can be delivered to users more quickly and efficiently. ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:2:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"CI/CD Tools options ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:3:0","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Jenkins pipeline { agent any stages { stage('Checkout') { steps { git branch: 'main', url: 'https://github.com/username/repo.git' } } stage('Build') { steps { sh 'pip install poetry' sh 'poetry install' } } stage('Test') { steps { sh 'poetry run pytest' } } stage('Deliver') { steps { echo 'Codebase approved, deliver to production environment' } } stage('Deploy') { steps { echo 'Deploy changes to production' // Add your deployment steps here } } } } ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:3:1","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"GitHub Actions name: CI/CD on: [push, pull_request] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: \"3.8\" - name: Build run: | pip install poetry poetry install - name: Test run: | poetry run pytest - name: Deliver run: | echo 'Codebase approved, deliver to production environment' - name: Deploy run: | echo 'Deploy changes to production' # Add your deployment steps here ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:3:2","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"CircleCI Visual Config Editor\rCircleCI has a Visual Config Editor which can help you visualize the pipeline.\rversion: 2.1 jobs: build: docker: - image: circleci/python:3.8 steps: - checkout - run: name: Build command: | pip install poetry poetry install - run: name: Test command: | poetry run pytest - run: name: Deliver command: | echo 'Codebase approved, deliver to production environment' - run: name: Deploy command: | echo 'Deploy changes to production' # Add your deployment steps here workflows: version: 2 build-deploy: jobs: - build ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:3:3","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Jenkins"],"content":"Tavis CI language: python python: - \"3.8\" install: - pip install poetry - poetry install script: - poetry run pytest after_success: - echo 'Codebase approved, deliver to production environment' - echo 'Deploy changes to production' # Add your deployment steps here Few more resources one can read: An Introduction to CI/CD Best Practices CI/CD concepts A beginner’s guide to CI/CD and automation on GitHub ","date":"2023-04-28","objectID":"/posts/devops/day11-devops/:3:4","tags":["DevOps","CI/CD","Jenkins","Tutorial","CircleCI","GitHub Actions","Travis CI"],"title":"Day 11: Introduction to CI/CD","uri":"/posts/devops/day11-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: GitHub Standard Fork \u0026 Pull Request Workflow Git Workflow | Atlassian Git Tutorial GitHub flow Some more resources which I found helpful: A list of cool features of Git and GitHub Git: Cheat Sheet (advanced) ","date":"2023-04-27","objectID":"/posts/devops/day10-devops/:0:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","GitHub","Remote Repository"],"title":"Day 10: Git workflows and best practices","uri":"/posts/devops/day10-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: About remote repositories - GitHub Docs Git remote : How to Collaborate - The Data Frog Collaboration is a key aspect of software development, and remote repositories enable developers to work together on the same project regardless of their location. In Git, a remote repository is a version of the codebase that is hosted on a server, allowing multiple developers to work on the same project simultaneously while keeping track of changes and ensuring that everyone is working with the most up-to-date code. In this blog post, we will discuss the process of setting up and using a remote repository in Git. ","date":"2023-04-26","objectID":"/posts/devops/day9-devops/:0:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","GitHub","Remote Repository"],"title":"Day 9: Remote repositories and collaboration with Git","uri":"/posts/devops/day9-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Setting Up a Remote Repository To set up a remote repository, you can use a service like GitHub or GitLab. The process for setting up a remote repository will vary depending on the platform you choose. Once you have set up the remote repository, you can add it to your local repository using the following command: git remote add origin \u003cremote repository URL\u003e The remote repository URL can be in SSH or HTTPS format. However, for SSH, you may need to set up a few additional configurations. ","date":"2023-04-26","objectID":"/posts/devops/day9-devops/:1:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","GitHub","Remote Repository"],"title":"Day 9: Remote repositories and collaboration with Git","uri":"/posts/devops/day9-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Pushing Changes to the Remote Repository Once you have set up the remote repository, you can push changes to it using the git push command: git push -u origin main In this command, origin is the default name given to the remote repository, but you can set it to any name you want. The -u flag tells Git to remember the upstream branch, and main is the branch that you want to push changes to. ","date":"2023-04-26","objectID":"/posts/devops/day9-devops/:2:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","GitHub","Remote Repository"],"title":"Day 9: Remote repositories and collaboration with Git","uri":"/posts/devops/day9-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Cloning a Remote Repository Most of the time, you will be working on a project that is already hosted in a remote repository. To clone the existing project to your local machine, you can use the git clone command: git clone \u003cremote repository URL\u003e This will create a local copy of the remote repository on your machine. ","date":"2023-04-26","objectID":"/posts/devops/day9-devops/:3:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","GitHub","Remote Repository"],"title":"Day 9: Remote repositories and collaboration with Git","uri":"/posts/devops/day9-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Fetching and Merging Changes from the Remote Repository Over time, the main branch in the remote repository will be updated with changes made by other developers. To update your local branch with the changes in the remote repository, you can use the git pull command: git pull origin main This command will fetch the changes from the remote repository and merge them into your local branch. ","date":"2023-04-26","objectID":"/posts/devops/day9-devops/:4:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","GitHub","Remote Repository"],"title":"Day 9: Remote repositories and collaboration with Git","uri":"/posts/devops/day9-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Pull Requests or Merge Requests A pull request (GitHub) or merge request (GitLab) is a way to propose changes from your branch to the main branch of the remote repository. This allows other collaborators to review your changes before they are merged into the main branch. To create a pull request or merge request, go to the remote repository’s web interface and follow the prompts. In conclusion, remote repositories are an essential part of collaboration in software development. By following the steps outlined in this blog post, you can set up and use a remote repository in Git to collaborate with other developers on your project. There is a nice interactive Git Cheatsheet which one can refer to understand the command used in last few post: Git Cheatsheet by ndpsoftware. ","date":"2023-04-26","objectID":"/posts/devops/day9-devops/:5:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","GitHub","Remote Repository"],"title":"Day 9: Remote repositories and collaboration with Git","uri":"/posts/devops/day9-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: Git - Basic Branching and Merging Git Merge | Atlassian Git Tutorial Branching and merging are essential features in Git that allow developers to work on multiple features or bug fixes simultaneously without affecting the main codebase. Git is a version control system that allows you to keep track of changes to your codebase and collaborate with other developers. Git’s branching feature allows you to create a new branch and make changes to the codebase without affecting the main branch. This feature is particularly useful when working on new features or bug fixes that are not yet ready for the main codebase. To create a new branch, use the command: git checkout -b new-feature This command will create a new branch named new-feature, and you can start making changes to the codebase in this branch. You can create as many branches as you need for your project. Once you have made the necessary changes to your codebase in the new branch, you can merge your changes back into the main branch. This can be done using the following commands: # Add all your changes git add . # Commit all of your changes git commit -m \"Commit message\" # Checkout to the branch where you want to merge the changes ","date":"2023-04-25","objectID":"/posts/devops/day8-devops/:0:0","tags":["DevOps","Git","GitHub","Tutorial","How-to","Branching","Merging"],"title":"Day 8: Branching and merging in Git","uri":"/posts/devops/day8-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance ","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:0:0","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"git init init is the subcommand to initialize a new Git repository. Depending on where you run the command, it will create a new repository in the current working directory. This creates a .git folder in the directory, and then you can use other Git commands and Git will track the changes. In case you want to use git init in some other directory, you can pass the path to git init like git init path/to/repository-working-directory. git init --bare\rGenerally, source code hosting sites like GitHub use bare repository. The repo in this case is initialized using git init --bare. To know more, check this SO post.\rgit init --separate-git-dir\rBy default, the .git folder stores any changes made in the directory where it exists, but we can actually use it to track a different working directory using the --separate-git-dir option, like git init --separate-git-dir=/path/to/repos.\rgit init --initial-branch\rYou can also set the initial branch using the --initial-branch option, like git init --initial-branch=my-initial-branch.\rgit init --initial-branch\rThere are other options like --template, which copies the default file and directory in the template folder, and --shared, which can be used to specify permissions.\r","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:1:0","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"git add add is the subcommand to stage changes made to the working directory for the next commit. Syntax: git add [file or folder] To add all files in the current working directory, use the git add . command. git add -i\rThere is also an interactive mode that you can access using git add -i, which presents you with a menu of options that allow you to: status: Show the current status of your working directory. update: Add modified files to the staging area. revert: Unstage files from the staging area. add untracked: Add untracked files to the staging area. patch: Interactively choose hunks of patch between the index and the work tree and add them to the index. git add -p\rIf your file has multiple changes and you want to stage only certain changes, you can use git add -p \u003cpath/to/file\u003e. This gives you an interactive shell where you can selectively stage changes within a file.\rgit add :/\rSometimes you might be in a different folder than your root directory and you want to add all the files that are changed. In that case, you can use the git add :/ command.\r","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:2:0","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"git commit commit is the subcommand used to save changes to the local repository. Syntax: git commit -m \"Commit Message\" git commit --allow-empty\rMost of the time, you will see git add . followed by git commit -m \"commit message\". There is a shorthand for this using git commit -am \"commit message\".\rgit commit --allow-empty\rSometimes, your pull request pipeline may fail due to some initialization issues. If you don’t have any way to re-trigger the pipeline using UI options in tools like GitHub Actions, Jenkins, or any other tool you are using, you can use git commit --allow-empty -m \"Empty commit to retrigger PR pipeline\". This can also be used to mark certain phases of your project development, like git commit --allow-empty -m \"Empty commit to mark the end of phase 1\". Note that this creates clutter and should only be used if you really need it. git commit --amend\rSometimes you mess up while writing a commit message and want to modify the last commit message. Or maybe you committed but forgot to add some changes which you intended to add before committing. You can use git commit --amend to modify the last commit message, including additional changes, instead of creating a new commit like git commit --amend -m \"New commit message\".\r","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:3:0","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"git status git status is the subcommand which displays the current status of a Git repository. The output shows which branch you’re currently on, which files have been modified, which files are staged for commit, and which files are not tracked by Git. Here are some scenarios you might see in git status. ","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:4:0","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Scenario 1: No changes $ git status On branch master nothing to commit, working tree clean ","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:4:1","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Scenario 2: Untacked files $ git status On branch master Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) file.txt nothing added to commit but untracked files present (use \"git add\" to track) ","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:4:2","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Scenario 3: Changes to be committed $ git status On branch master Changes to be committed: (use \"git restore --staged \u003cfile\u003e...\" to unstage) modified: file.txt ","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:4:3","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Scenario 4: Changes not staged for commit $ git status On branch master Changes not staged for commit: (use \"git add \u003cfile\u003e...\" to update what will be committed) (use \"git restore \u003cfile\u003e...\" to discard changes in working directory) modified: file.txt no changes added to commit (use \"git add\" and/or \"git commit -a\") ","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:4:4","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"Scenario 5: Changes in branch ahead of remote $ git status On branch master Your branch is ahead of 'origin/master' by 1 commit. (use \"git push\" to publish your local commits) nothing to commit, working tree clean git status --short\rgit status --short let you display a compact version of git status.\rgit status --ignore-submodules\rIn case you have submodules and you don’t want to display status about those, use git status --ignore-submodules.\rHere is a sequence digram to wrap it all up This diagram illustrates the following steps: git init initializes the workspace as a Git repository. Files are created or modified in the workspace. git status shows untracked or modified files in the workspace. git add \u003cfile\u003e stages changes for a file, adding it to the index. git status shows staged changes in the workspace. git commit -m \"Message\" commits the staged changes to the local repository. git status shows a clean working tree, indicating that there are no changes to be committed. ","date":"2023-04-22","objectID":"/posts/devops/day7-devops/:4:5","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 7: Basic Git commands (`git init`, `git add`, `git commit`, `git status`)","uri":"/posts/devops/day7-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure","Git"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: Introduction to Git - Training | Microsoft Learn Git - gittutorial Documentation Learn Git - Tutorials, Workflows and Commands | Atlassian ","date":"2023-04-21","objectID":"/posts/devops/day6-devops/:0:0","tags":["DevOps","Git","GitHub","Tutorial","How-to"],"title":"Day 6: Introduction to Git","uri":"/posts/devops/day6-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: Devops Best Practices Checklist · GitHub bregman-arie/devops-exercises codeaprendiz/learn-devops AdamPaternostro/DevOps-Best-Practices Tikam02/DevOps-Guide ","date":"2023-04-21","objectID":"/posts/devops/day5-devops/:0:0","tags":["DevOps","Best-Practice","GitHub Documentation","Tutorial","How-to"],"title":"Day 5: Studying DevOps culture and best practices","uri":"/posts/devops/day5-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: Awesome DevOps The Tools and Technology of DevOps | Smartsheet ","date":"2023-04-19","objectID":"/posts/devops/day4-devops/:0:0","tags":["DevOps","Docker","Kubernetes","Jenkins","CI/CD","Monitoring","Automation","Tutorial","How-to"],"title":"Day 4: Familiarizing with common DevOps tools and technologies","uri":"/posts/devops/day4-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: CI/CD concepts | GitLab Continuous Integration and Continuous Delivery (CI/CD) Fundamentals … A beginner’s guide to CI/CD and automation on GitHub ","date":"2023-04-18","objectID":"/posts/devops/day3-devops/:0:0","tags":["DevOps","Docker","Kubernetes","Jenkins","CI/CD","Monitoring","Automation","Tutorial","How-to"],"title":"Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD)","uri":"/posts/devops/day3-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Resource I refered: DevOps Lifecycle: 7 Phases Explained in Detail with Examples - Simform ","date":"2023-04-17","objectID":"/posts/devops/day2-devops/:0:0","tags":["DevOps","Docker","Kubernetes","Git","CI/CD","Tutorial","How-to"],"title":"Day 2: Exploring the DevOps lifecycle and its stages","uri":"/posts/devops/day2-devops/"},{"categories":["DevOps","Software Development","Automation","Infrastructure"],"content":"\rContent\rPart 1: Introduction to DevOps Day 1: Understanding DevOps, its principles, and benefits Day 2: Exploring the DevOps lifecycle and its stages Day 3: Introduction to Continuous Integration (CI) and Continuous Deployment (CD) Day 4: Familiarizing with common DevOps tools and technologies Day 5: Studying DevOps culture and best practices Part 2: Version Control Systems Day 6: Introduction to Git Day 7: Basic Git commands (git init, git add, git commit, git status) Day 8: Branching and merging in Git Day 9: Remote repositories and collaboration with Git Day 10: Git workflows and best practices Part 3: Continuous Integration and Continuous Deployment (CI/CD) Day 11: Introduction to CI/CD Day 12: Jenkins - Installation and configuration Day 13: Jenkins - Creating and managing jobs Day 14: Jenkins - Integrating with Git Day 15: Jenkins - Pipelines and best practices Part 4: Configuration Management Day 16: Introduction to configuration management Day 17: Ansible - Installation and configuration Day 18: Ansible - Ad-hoc commands and playbooks Day 19: Ansible - Roles and best practices Day 20: Puppet and Chef - Overview and comparison Part 5: Infrastructure as Code Day 21: Introduction to Infrastructure as Code (IaC) Day 22: Terraform - Installation and configuration Day 23: Terraform - Writing and applying configuration files Day 24: Terraform - Modules and best practices Day 25: CloudFormation (AWS) - Overview and comparison Part 6: Containerization Day 26: Introduction to containerization Day 27: Docker - Installation and configuration Day 28: Docker - Building and managing images Day 29: Docker - Running and managing containers Day 30: Docker Compose and best practices Part 7: Container Orchestration Day 31: Introduction to container orchestration Day 32: Kubernetes - Architecture and components Day 33: Kubernetes - Deployments, services, and storage Day 34: Kubernetes - ConfigMaps and secrets Day 35: Kubernetes - Best practices and Helm Part 8: Monitoring and Logging Day 36: Introduction to monitoring and logging Day 37: Prometheus - Installation and configuration Day 38: Prometheus - Querying and alerting Day 39: Grafana - Installation and configuration Day 40: ELK Stack (Elasticsearch, Logstash, Kibana) - Overview and comparison Part 9: Cloud Platforms Day 41: Introduction to cloud platforms Day 42: AWS - EC2, S3, and RDS Day 43: AWS - IAM, VPC, and ELB Day 44: Azure - Virtual Machines, Storage, and SQL Database Day 45: Google Cloud Platform - Compute Engine, Storage, and Cloud SQL Part 10: DevOps Security Day 46: Introduction to DevOps security Day 47: Security best practices for CI/CD pipelines Day 48: Infrastructure and application security Day 49: Container and Kubernetes security Day 50: Cloud security and compliance Despite holding a full-time job, I am currently dedicating time to document my learning in DevOps in a structured manner. While I have gained significant practical experience in DevOps through my daily responsibilities, I have not yet had the opportunity to systematically document my learnings. Through this series, my aim is to create comprehensive and organized notes that I can reference in the future. I plan to collect various resources available online and supplement them with my own insights and summaries. In order to deepen my understanding of the core concepts and principles of DevOps, I have found the following resources to be particularly helpful: What is DevOps? DevOps Explained | Microsoft Azure What is DevOps? - Azure DevOps | Microsoft Learn DevOps Principles | Atlassian Benefits of DevOps | Atlassian The majority of the principles discussed encompass the following key tenets: Firstly, a collaborative approach between development, operations, and other stakeholders in the software development process is crucial. Secondly, the automation of repetitive tasks such as testing, deployment, and infrastructure management is paramount. This, in turn, improves efficiency and reduces the likelihood o","date":"2023-04-16","objectID":"/posts/devops/day1-devops/:0:0","tags":["DevOps","Docker","Kubernetes","Git","Jenkins","CI/CD","Monitoring","Automation","Tutorial","How-to"],"title":"Day 1: Understanding DevOps, its principles, and benefits","uri":"/posts/devops/day1-devops/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Convert Sorted Array to Binary Search Tree Link to original Problem on LeetCode Given an array where elements are sorted in ascending order, convert it to a height balanced BST. For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Example: Given the sorted array: [-10,-3,0,5,9], One possible answer is: [0,-3,9,-10,null,5], which represents the following height balanced BST: Company: Amazon, VMWare Solution (Using Recursion): Time Complexity: Space Complexity: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def sortedArrayToBST(self, nums): \"\"\" :type nums: List[int] :rtype: TreeNode \"\"\" # The idea is in each loop tale the middle element # Make an object of TreeNode with value as the middle element # Now all the values left to this middle element will be part of the left sub tree # Likewise all the values right to this middle element will be part of right sub tree # We have to recursively call this method and assigne the middle element of all the value to left of current middle element to the left node # Same for the right node if not nums: return None mid = len(nums) // 2 root = TreeNode(nums[mid]) root.left = self.sortedArrayToBST(nums[:mid]) root.right = self.sortedArrayToBST(nums[mid + 1:]) return root Solution (Using Recursion - helper method): Time Complexity: Space Complexity: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None class Solution(object): def sortedArrayToBST(self, nums): \"\"\" :type nums: List[int] :rtype: TreeNode \"\"\" # Here concept is same but we are using a helper method if not nums: return None return self.helper(nums, 0, len(nums)) def helper(self, nums, left, right): if left \u003e= right: return None mid = (left + right) // 2 root = TreeNode(nums[mid]) root.left = self.helper(nums, left, mid) root.right = self.helper(nums, mid + 1, right) return root \u003chr\u003e\u003cbr /\u003e ","date":"2019-09-06","objectID":"/posts/leetcode/convert-sorted-array-to-binary-search-tree/:0:1","tags":["Algorithms","Binary Search Tree","Conversion","Programming"],"title":"[LeetCode] 108. Convert Sorted Array to Binary Search Tree","uri":"/posts/leetcode/convert-sorted-array-to-binary-search-tree/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Shuffle an Array Link to original Problem on LeetCode Shuffle a set of numbers without duplicates. Example: // Init an array with set 1, 2, and 3.\rint[] nums = {1,2,3};\rSolution solution = new Solution(nums);\r// Shuffle the array [1,2,3] and return its result. Any permutation of [1,2,3] must equally likely to be returned.\rsolution.shuffle();\r// Resets the array back to its original configuration [1,2,3].\rsolution.reset();\r// Returns the random shuffling of array [1,2,3].\rsolution.shuffle(); Company: Facebook Solution Time Complexity: Space Complexity: class Solution(object): # This solution is based on python inbuild random method def __init__(self, nums): \"\"\" :type nums: List[int] \"\"\" self.array = nums self.original = nums[:] def reset(self): \"\"\" Resets the array to its original configuration and return it. :rtype: List[int] \"\"\" self.array = self.original[:] return self.array def shuffle(self): \"\"\" Returns a random shuffling of the array. :rtype: List[int] \"\"\" for i in range(len(self.array)): swapIndex = random.randrange(0, len(self.array)) self.array[i], self.array[swapIndex] = self.array[swapIndex], self.array[i] return self.array # Your Solution object will be instantiated and called as such: # obj = Solution(nums) # param_1 = obj.reset() # param_2 = obj.shuffle() ","date":"2019-09-06","objectID":"/posts/leetcode/shuffle-an-array/:0:1","tags":["Algorithms","Shuffle","Arrays","Randomization","Programming"],"title":"[LeetCode] 384. Shuffle an Array","uri":"/posts/leetcode/shuffle-an-array/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Excel Sheet Column Number Link to original Problem on LeetCode Given a column title as appear in an Excel sheet, return its corresponding column number. For example: Example 1: Input: “A” Output: 1 Example 2: Input: “AB” Output: 28 Example 3: Input: “ZY” Output: 701 Company: Amazon Solution (Using Hash Map): Time Complexity: O(n) Space Complexity: O(1) class Solution(object): def titleToNumber(self, s): \"\"\" :type s: str :rtype: int \"\"\" # First create a dictionary containing key 'A' to 'Z' # assigning 'A': 1, 'B': 2, 'C': 3, ..., 'Z': 26 # For element in position i, value = (26 ^ i) x (character number) # eg: if A is in 3rd position from left, # value = (26 ^ 2) x 1 = 676 # We just need to add up all the values charDict = {chr(x): x - 64 for x in range(65, 65 + 26)} columnNumber = 0 for i, char in enumerate(reversed(s)): columnNumber += 26 ** i * charDict[char] return columnNumber ","date":"2019-09-05","objectID":"/posts/leetcode/excel-sheet-column-number/:0:1","tags":["Algorithms","Excel","Column Number","Programming"],"title":"[LeetCode] 171. Excel Sheet Column Number","uri":"/posts/leetcode/excel-sheet-column-number/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Kth Smallest Element in a BST Link to original Problem on LeetCode Given a binary search tree, write a function kthSmallest to find the kth smallest element in it. Note: You may assume k is always valid, 1 ≤ k ≤ BST’s total elements. Example 1: Input: root = [3,1,4,null,2], k = 1 Output: 1 Example 2: Input: root = [5,3,6,2,4,null,null,1], k = 3 Output: 3 Follow up: What if the BST is modified (insert/delete operations) often and you need to find the kth smallest frequently? How would you optimize the kthSmallest routine? Company: Amazon Solution (Using stack): Time Complexity: O(h + k) # Were h is height of the tree Space Complexity: O(h + k) # Were h is height of the tree # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def kthSmallest(self, root, k): \"\"\" :type root: TreeNode :type k: int :rtype: int \"\"\" # We can keep all the left node to a stack till we can't find any left node stack = [] while root: stack.append(root) root = root.left # Now in a loop we will decreament k till it reaches 0 while k: k -= 1 # We pop the last node inserted # (Note: For 1st iteration, this is the 1st smallest node, for 2nd iteration we get 2nd smallest node, and so on) currNode = stack.pop() # If k == 0, then we got our kth element if k == 0: return currNode.val # If the current node has right node, we have to push it to the stack, followed by all the left node of that node # This is because for any node in a stack, it is already the left most node (smallest), # But next pop element would be parent of that node which will be greater than the node to the right of current pop node # For example, In example 1, the stack will have | 3 | 1 |, # On first iteration we will pop 1, now we have to push all the left node in right sub tree of 1, here we have only one i.e. 2 # So stack after insertion of all left node in right sub tree will be | 3 | 2 | right = currNode.right while right: stack.append(right) right = right.left Solution (Using Recursion): Time Complexity: O(n) Space Complexity: O(n) # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def kthSmallest(self, root, k): \"\"\" :type root: TreeNode :type k: int :rtype: int \"\"\" # We can create array using in-order traversal # Inorder traversal of binary tree is a sorted array # We can just return the k - 1, since index of array starts at 0 in Python return self.inOrder(root)[k - 1] def inOrder(self, root): return self.inOrder(root.left) + [root.val] + self.inOrder(root.right) if root else [] ","date":"2019-09-05","objectID":"/posts/leetcode/kth-smallest-element-in-a-bst/:0:1","tags":["Algorithms","Binary Search Tree","Kth Smallest","Programming"],"title":"[LeetCode] 230. Kth Smallest Element in a BST","uri":"/posts/leetcode/kth-smallest-element-in-a-bst/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Best Time to Buy and Sell Stock II Link to original Problem on LeetCode Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times). Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Example 1: Input: [7,1,5,3,6,4] Output: 7 Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4. Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3. Example 2: Input: [1,2,3,4,5] Output: 4 Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4. Note that you cannot buy on day 1, buy on day 2 and sell them later, as you are engaging multiple transactions at the same time. You must sell before buying again. Example 3: Input: [7,6,4,3,1] Output: 0 Explanation: In this case, no transaction is done, i.e. max profit = 0. Company: Amazon Solution Time Complexity: O(n) Space Complexity: O(1) class Solution(object): def maxProfit(self, prices): \"\"\" :type prices: List[int] :rtype: int \"\"\" # We solve this problem using greedy solution # Every time we see price of the stock which is less than the price of stock next day, # We buy and sell the stock and add it up to the profit profit = 0 for i in range(len(prices) - 1): if prices[i] \u003c prices[i + 1]: profit += prices[i + 1] - prices[i] return profit ","date":"2019-09-04","objectID":"/posts/leetcode/best-time-to-buy-and-sell-stock-ii/:0:1","tags":["Algorithms","Stock Market","Optimization","Programming"],"title":"[LeetCode] 122. Best Time to Buy and Sell Stock II","uri":"/posts/leetcode/best-time-to-buy-and-sell-stock-ii/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Contains Duplicate Link to original Problem on LeetCode Given an array of integers, find if the array contains any duplicates. Your function should return true if any value appears at least twice in the array, and it should return false if every element is distinct. Example 1: Input: [1,2,3,1] Output: true Example 2: Input: [1,2,3,4] Output: false Example 3: Input: [1,1,1,3,3,4,3,2,4,2] Output: true Company: Amazon Solution (Using Sorting): Time Complexity: O(n log n) Space Complexity: O(1) class Solution(object): def containsDuplicate(self, nums): \"\"\" :type nums: List[int] :rtype: bool \"\"\" # We know that if there are 1 or no element than we don't have any duplicate if len(nums) \u003c= 1: return False # Sort the array nums.sort() # Check if any two adjacent elemets are same # If yes then return True, since duplicate element is present for i,num in enumerate(nums): if nums[i - 1] == num: return True return False Solution (Using Hashing): Time Complexity: O(n) Space Complexity: O(n) class Solution(object): def containsDuplicate(self, nums): \"\"\" :type nums: List[int] :rtype: bool \"\"\" # Maintain a set. For each element check whether it is already present in the set # If not then add it, else return True as we found that we already have same element unique = set() for num in nums: if num in unique: return True else: unique.add(num) return False ","date":"2019-09-04","objectID":"/posts/leetcode/contains-duplicate/:0:1","tags":["Algorithms","Duplicates","Arrays","Programming"],"title":"[LeetCode] 217. Contains Duplicate","uri":"/posts/leetcode/contains-duplicate/"},{"categories":["Algorithms","Programming","LeetCode","Interview Preparation"],"content":"Top Programming Questions [Updated] ","date":"2019-09-04","objectID":"/posts/leetcode/top-interview-questions/:0:1","tags":["Top Interview Questions","LeetCode","Algorithm","Python","Problem Solving","Interview Preparation"],"title":"Top Programming Questions [Updated]","uri":"/posts/leetcode/top-interview-questions/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Roman to Integer Link to original Problem on LeetCode Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Symbol Value I 1 V 5 X 10 L 50 C 100 D 500 M 1000 For example, two is written as II in Roman numeral, just two one’s added together. Twelve is written as, XII, which is simply X + II. The number twenty seven is written as XXVIIi, which is XX + V + II. Roman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine, which is written as IX. There are six instances where subtraction is used: I can be placed before V (5) and X (10) to make 4 and 9. X can be placed before L (50) and C (100) to make 40 and 90. C can be placed before D (500) and M (1000) to make 400 and 900. Given a roman numeral, convert it to an integer. Input is guaranteed to be within the range from 1 to 3999. Example 1: Input: “III” Output: 3 Example 2: Input: “IV” Output: 4 Example 3: Input: “IX” Output: 9 Example 4: Input: “LVIII” Output: 58 Explanation: L = 50, V= 5, III = 3. Example 5: Input: “MCMXCIV” Output: 1994 Explanation: M = 1000, CM = 900, XC = 90 and IV = 4. Company: Yahoo, Google, Amazon, Microsoft Solution Time Complexity: O(n) Space Complexity: O(1) # Since the dictionary only takes a constant space Solution(object): def romanToInt(self, s): \"\"\" :type s: str :rtype: int \"\"\" # If we check the Roman number, there are basically 2 things which is happening # 1. Number correponding to the last letter is always added # 2. Except point 1. if the number corresponding to a letter is less than the number # corresponding to the letter next to that, then it will be subtracted, else added romanToIntDict = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000} result = 0 for i in range(len(s) - 1): # Using 2. if romanToIntDict[s[i]] \u003c romanToIntDict[s[i + 1]]: result += -romanToIntDict[s[i]] else: result += romanToIntDict[s[i]] # Using 1. result += romanToIntDict[s[-1]] return result ","date":"2019-08-28","objectID":"/posts/leetcode/roman-to-integer/:0:1","tags":["Algorithms","Roman Numerals","Conversion","Programming"],"title":"[LeetCode] 13. Roman to Integer","uri":"/posts/leetcode/roman-to-integer/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Majority Element Link to original Problem on LeetCode Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Example 1: Input: [3,2,3] Output: 3 Example 2: Input: [2,2,1,1,1,2,2] Output: 2 Company: Yahoo, Google, Amazon, Microsoft Solution (Using Hashing - 2 pass): Time Complexity: O(n) Space Complexity: O(n) class Solution(object): def majorityElement(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # Here the solution is to keep a dictionary of values and there count in nums elementDict = {} halfLength = len(nums) // 2 for num in nums: elementDict[num] = elementDict.get(num, 0) + 1 # Then we can go through the dictionary and check for the value whose count is more than len(nums) / 2 for key, value in elementDict.items(): if value \u003e halfLength: return key Solution (Using Hashing - 1 pass): Time Complexity: O(n) Space Complexity: O(n) class Solution(object): def majorityElement(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" elementDict = {} halfLength = len(nums) // 2 for num in nums: elementDict[num] = elementDict.get(num, 0) + 1 if elementDict[num] \u003e halfLength: return num Solution (Sorting): Time Complexity: O(n log n) Space Complexity: O(1) class Solution(object): def majorityElement(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # First sort the list nums.sort() # Now after sorting the element which is present in middle will be the majority element return nums[len(nums) / 2] ","date":"2019-08-27","objectID":"/posts/leetcode/majority-element/:0:1","tags":["Algorithms","Majority Element","Moore's Voting Algorithm","Programming"],"title":"[LeetCode] 169. Majority Element","uri":"/posts/leetcode/majority-element/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Subsets Link to original Problem on LeetCode Given a set of distinct integers, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. Example: Input: nums = [1,2,3] Output: [ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], [] ] Company: Amazon, Microsoft Solution (Using Iteration): Time Complexity: Space Complexity: class Solution(object): def subsets(self, nums): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" # Let's take an example, say we have nums = [1, 2, 3] # Initially we have an empty subset in the result i.e. result = [[]] # Adding [1] to each element of result (i.e. [[]], one element) # [] + [1] = [1] # we have result = [[], [1]] # # Adding [2] to each element of result (i.e.[[], [1]], two element) # [] + [2] = [2] # [1] + [2] = [1, 2] # we have result = [[], [1], [2], [1, 2]] # # Adding [] to each element of result (i.e.[[], [1], [2], [1, 2]], four element) # [] + [3] = [3] # [1] + [3] = [1, 3] # [2] + [3] = [2, 3] # [1, 2] + [3] = [1, 2, 3] # we have result = [[], [1], [2], [1, 2], [3], [1, 3], [2, 3], [1, 2, 3]] result = [[]] for num in nums: temp = result[:] for res in temp: result.append(res + [num]) return result Solution (Using Recursion): Time Complexity: Space Complexity: class Solution(object): def subsets(self, nums): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" # Let's take the same example, say we have nums = [1, 2, 3] # Initially we have an empty subset in the result i.e. result = [[]] # Adding [3] to each element of result (i.e. [[]], one element) # [3] + [] = [3] # we have result = [[], [3]] # # Adding [2] to each element of result (i.e.[[], [3]], two element) # [2] + [] = [2] # [2] + [3] = [2, 3] # we have result = [[], [3], [2], [2, 3]] # # Adding [1] to each element of result (i.e.[[], [3], [2], [2, 3], four element) # [1] + [] = [1] # [1] + [3] = [1, 3] # [1] + [2] = [1, 2] # [1] + [2, 3] = [1, 2, 3] # we have result = [[], [3], [2], [2, 3], [1], [1, 3], [1, 2], [1, 2, 3]] # This is kinda iterative approch which can be implemented rescursively result = [] self.subsetsHelper(nums, [], 0, result) return result def subsetsHelper(self, nums, subset, i, result): if i == len(nums): result.append(subset) return self.subsetsHelper(nums, subset, i + 1, result) self.subsetsHelper(nums, subset + [nums[i]], i + 1, result) ","date":"2019-08-08","objectID":"/posts/leetcode/subsets/:0:1","tags":["Algorithms","Subsets","Combinations","Backtracking","Recursion","Programming"],"title":"[LeetCode] 78. Subsets","uri":"/posts/leetcode/subsets/"},{"categories":["Programming","Algorithms","Python"],"content":"Top K Frequent Elements Link to original Problem on LeetCode Given a non-empty array of integers, return the k most frequent elements. Example 1: Input: nums = [1,1,1,2,2,3], k = 2 Output: [1,2] Example 2: Input: nums = [1], k = 1 Output: [1] Note: You may assume k is always valid, 1 ≤ k ≤ number of unique elements. Your algorithm’s time complexity must be better than O(n log n), where n is the array’s size. Company: Amazon Solution (Using Python inbuilt feature - Counter): Time Complexity: O(n log n) Space Complexity: O(n) import collections class Solution(object): def topKFrequent(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: List[int] \"\"\" # Counter(nums) return a Counter object which have key: value pairs of all the items: count of item. # most_common(k) method will return a list of tupple containing k most common element with their count return [x[0] for x in collections.Counter(nums).most_common(k)] Internally Counter().most_common(k) uses heapq.nlargest() Solution using heapq: Time Complexity: O(n log n) Space Complexity: O(n) import heapq class Solution(object): def topKFrequent(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: List[int] \"\"\" numCount = {} for num in nums: numCount[num] = numCount.get(num, 0) + 1 return heapq.nlargest(k, numCount, key=lambda x: numCount[x]) ","date":"2019-08-02","objectID":"/posts/leetcode/top-k-frequent-elements/:0:1","tags":["Top K","Frequent Elements","Algorithm","Python","Tutorial"],"title":"[LeetCode] 347. Top K Frequent Elements","uri":"/posts/leetcode/top-k-frequent-elements/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Delete Node in a Linked List Link to original Problem on LeetCode Write a function to delete a node (except the tail) in a singly linked list, given only access to that node. Given linked list – head = [4,5,1,9], which looks like following: Example 1: Input: head = [4,5,1,9], node = 5 Output: [4,1,9] Explanation: You are given the second node with value 5, the linked list should become 4 -\u003e 1 -\u003e 9 after calling your function. Example 2: Input: head = [4,5,1,9], node = 1 Output: [4,5,9] Explanation: You are given the third node with value 1, the linked list should become 4 -\u003e 5 -\u003e 9 after calling your function. Note: The linked list will have at least two elements. All of the nodes’ values will be unique. The given node will not be the tail and it will always be a valid node of the linked list. Do not return anything from your function. Company: Amazon, Goldman Sachs, Microsoft, Samsung, Visa Solution: Time Complexity: O(1) Space Complexity: O(1) class Solution(object): def deleteNode(self, node): \"\"\" :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. \"\"\" # You have been given the reference to the node which is to be deleted # The simple way to delete is to: # -\u003e change the value of the current node with the value of next node # -\u003e again the reference to the next attribute is to be changed with the reference of next to next node node.val, node.next = node.next.val, node.next.next ","date":"2019-07-25","objectID":"/posts/leetcode/delete-node-in-a-linked-list/:0:1","tags":["Delete Node in a Linked List","Linked List","LeetCode","Algorithm"],"title":"[LeetCode] 237. Delete Node in a Linked List","uri":"/posts/leetcode/delete-node-in-a-linked-list/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Move Zeroes Link to original Problem on LeetCode Given an array nums, write a function to move all 0’s to the end of it while maintaining the relative order of the non-zero elements. Example: Input: [0,1,0,3,12] Output: [1,3,12,0,0] Note: You must do this in-place without making a copy of the array. Minimize the total number of operations. Company: Amazon, Bloomberg, Paytm, Samsung Solution: Time Complexity: O(n) Space Complexity: O(1) class Solution(object): def moveZeroes(self, nums): \"\"\" :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. \"\"\" # One solution will be to keep track of where to place the next non-zero number # We have to start with index 0 index = 0 for i in range(len(nums)): if nums[i] != 0: nums[index] = nums[i] index += 1 # Once we have traverse through the whole list we now basically have count of the non-zero number in index # i.e. number of non-zero element = index # and number of zeroes = len(nums) - index # So we fill all the remaining places (len(nums) - index) with zeroes for i in range(index, len(nums)): nums[i] = 0 Solution (Optimal): Time Complexity: O(n) Space Complexity: O(1) class Solution(object): def moveZeroes(self, nums): \"\"\" :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. \"\"\" # Another solution which is same as the previous one but more optimal # Instead of replacing the number at the particular index, # we can swap it with the one which is present in the index where we need to place non-zero number # We have to start with index 0 index = 0 for i in range(len(nums)): if nums[i] != 0: nums[index], nums[i] = nums[i], nums[index] index += 1 ","date":"2019-07-25","objectID":"/posts/leetcode/move-zeroes/:0:1","tags":["Move Zeroes","Two Pointers","Array","LeetCode","Algorithm"],"title":"[LeetCode] 283. Move Zeroes","uri":"/posts/leetcode/move-zeroes/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Reverse Linked List Link to original Problem on LeetCode Reverse a singly linked list. Example: Input: 1-\u003e2-\u003e3-\u003e4-\u003e5-\u003eNULL Output: 5-\u003e4-\u003e3-\u003e2-\u003e1-\u003eNULL Follow up: A linked list can be reversed either iteratively or recursively. Could you implement both? Company: Adobe, Amazon, MakeMyTrip, Microsoft, Qualcomm, Samsung Solution (Iterative): Time Complexity: Space Complexity: class Solution(object): def reverseList(self, head): \"\"\" :type head: ListNode :rtype: ListNode \"\"\" # Video Explanation: https://youtu.be/sYcOK51hl-A # Credit: mycodeschool prev = None curr = head while curr: next = curr.next curr.next = prev prev = curr curr = next return prev Solution (Recursive): class Solution(object): def reverseList(self, head): \"\"\" :type head: ListNode :rtype: ListNode \"\"\" # Logic behind recursion is same as that of the iterative version # We are just swapping the reference recursively return self.reverseListHelper(head, None) def reverseListHelper(self, curr, prev): if curr is None: return prev next = curr.next curr.next = prev return self.reverseListHelper(next, curr) ","date":"2019-07-24","objectID":"/posts/leetcode/reverse-linked-list/:0:1","tags":["Reverse Linked List","Linked List","LeetCode","Algorithm"],"title":"[LeetCode] 206. Reverse Linked List","uri":"/posts/leetcode/reverse-linked-list/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Product of Array Except Self Link to original Problem on LeetCode Given an array nums of n integers where n \u003e 1, return an array output such that output[i] is equal to the product of all the elements of nums except nums[i]. Example: Input: [1,2,3,4] Output: [24,12,8,6] Note: Please solve it without division and in O(n). Follow up: Could you solve it with constant space complexity? (The output array does not count as extra space for the purpose of space complexity analysis.) Company: Accolite, Amazon, D-E-Shaw, Morgan Stanley, Opera Solution: Time Complexity: Space Complexity: class Solution(object): def productExceptSelf(self, nums): \"\"\" :type nums: List[int] :rtype: List[int] \"\"\" output = [1] * len(nums) for i in range(1, len(nums)): output[i] = output[i - 1] * nums[i - 1] print output multiplyThis = nums[-1] for i in range(len(nums) - 2, -1, -1): output[i] = output[i] * multiplyThis multiplyThis *= nums[i] return output ","date":"2019-07-24","objectID":"/posts/leetcode/product-of-array-except-self/:0:1","tags":["Product of Array Except Self","Array","LeetCode","Algorithm"],"title":"[LeetCode] 238. Product of Array Except Self","uri":"/posts/leetcode/product-of-array-except-self/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Maximum Depth of Binary Tree Link to original Problem on LeetCode Given a binary tree, find its maximum depth. The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node. Note: A leaf is a node with no children. Example: Given binary tree [3,9,20,null,null,15,7], return its depth = 3. Company: Goldman Sachs, Facebook, Bloomberg, Microsoft Solution (Iterative using breadth first search): class Solution(object): def maxDepth(self, root): \"\"\" :type root: TreeNode :rtype: int \"\"\" # Do BFS and at each level increase the count if root is None: return 0 queue, count = [root], 0 while queue: count += 1 for i in range(len(queue)): node = queue.pop(0) if node.left: queue.append(node.left) if node.right: queue.append(node.right) print return count Solution (Recursive using depth first search): class Solution(object): def maxDepth(self, root): \"\"\" :type root: TreeNode :rtype: int \"\"\" # Using DFS keep the max count in each recursive call if root == None: return 0 return 1 + max(self.maxDepth(root.left), self.maxDepth(root.right)) ","date":"2019-07-23","objectID":"/posts/leetcode/maximum-depth-of-binary-tree/:0:1","tags":["Maximum Depth of Binary Tree","Tree","Depth-First Search","LeetCode","Algorithm"],"title":"[Leetcode] 104. Maximum Depth of Binary Tree","uri":"/posts/leetcode/maximum-depth-of-binary-tree/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Single Number Link to original Problem on LeetCode Given a non-empty array of integers, every element appears twice except for one. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? Example 1: Input: [2,2,1] Output: 1 Example 2: Input: [4,1,2,1,2] Output: 4 Company: Amazon Solution (Using HashMap): Time Complexity: O(n) Space Complexity: O(n) class Solution(object): def singleNumber(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # Keep a dictionary count and at the end just check whether any number has a count of 1 numCount = {} for num in nums: numCount[num] = numCount.get(num, 0) + 1 for key, value in numCount.items(): if value == 1: return key Solution (Using Set): Time Complexity: O(n) Space Complexity: O(n) class Solution(object): def singleNumber(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # We know that every number except the number whose count is one is repeated twice # Therefore if we multiply all unique number with 2 and then subtract it with the sum of the list we should get our desire output return 2 * sum(set(nums)) - sum(nums) Solution (Using XOR): Time Complexity: O(n) Space Complexity: O(1) class Solution(object): def singleNumber(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # XOR of same elements is zero result = 0 for num in nums: result ^= num return result Solution (Using reduce method, only in Python): Time Complexity: O(n) Space Complexity: O(1) class Solution(object): def singleNumber(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # It will perform XOR operator with the list pass to it return reduce(operator.xor, nums) ","date":"2019-07-23","objectID":"/posts/leetcode/single-number/:0:1","tags":["Single Number","LeetCode","Bit Manipulation","XOR","Algorithm","Python","Problem Solving"],"title":"[LeetCode] 136. Single Number","uri":"/posts/leetcode/single-number/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Generate Parentheses Link to original Problem on LeetCode Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [ “((()))”, “(()())”, “(())()”, “()(())”, “()()()” ] Company: Microsoft, Facebook Solution (Backtracking and Recursion): Time Complexity: Space Complexity: class Solution(object): def generateParenthesis(self, n): \"\"\" :type n: int :rtype: List[str] \"\"\" result = [] self.generateParenthesisHelper(n, n, \"\", result) return result def generateParenthesisHelper(self, left, right, path, result): if left \u003e right: return if left == 0 and right == 0: result.append(path) if left: self.generateParenthesisHelper(left - 1, right, path + \"(\", result) if right: self.generateParenthesisHelper(left, right - 1, path + \")\", result) Explanation: Following is how the recursion is working: generateParenthesisHelper(2, 2, [], \"\")\rgenerateParenthesisHelper(1, 2, [], \"(\")\rgenerateParenthesisHelper(0, 2, [], \"((\")\rgenerateParenthesisHelper(0, 1, [], \"(()\")\rgenerateParenthesisHelper(0, 0, [], \"(())\") # We got \"(())\" and we append it to ans\rgenerateParenthesisHelper(1, 1, [\"(())\"], \"()\")\rgenerateParenthesisHelper(0, 1, [\"(())\"], \"()(\")\rgenerateParenthesisHelper(0, 0, [\"(())\"], \"()()\") # We got \"(())\" and we append it to ans\rgenerateParenthesisHelper(1, 0, [\"(())\", \"()()\"], \"())\") # will just return as right \u003c left\rgenerateParenthesisHelper(2, 1, [\"(())\", \"()()\"], \")\") # will just return as right \u003c left ","date":"2019-07-23","objectID":"/posts/leetcode/generate-parentheses/:0:1","tags":["Generate Parentheses","Backtracking","String","LeetCode","Algorithm"],"title":"[LeetCode] 22. Generate Parentheses","uri":"/posts/leetcode/generate-parentheses/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Reverse String Link to original Problem on LeetCode Write a function that reverses a string. The input string is given as an array of characters char[]. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. You may assume all the characters consist of printable ascii characters. Example 1: Input: [“h”,“e”,“l”,“l”,“o”] Output: [“o”,“l”,“l”,“e”,“h”] Example 2: Input: [“H”,“a”,“n”,“n”,“a”,“h”] Output: [“h”,“a”,“n”,“n”,“a”,“H”] Company: Solution (Python Way): class Solution(object): def reverseString(self, s): \"\"\" :type s: List[str] :rtype: None Do not return anything, modify s in-place instead. \"\"\" # Using [::-1] to reverse the string s[:] = s[::-1] Solution (Iterative): class Solution(object): def reverseString(self, s): \"\"\" :type s: List[str] :rtype: None Do not return anything, modify s in-place instead. \"\"\" # Keep two reference to the first and the last element of the list, say left and right respectively # Now until left is greater than right, swap left and right element and increament left and decreament right left, right = 0, len(s) - 1 while left \u003c= right: s[left], s[right] = s[right], s[left] left += 1 right -= 1 Solution (Recursive): class Solution(object): def reverseString(self, s): \"\"\" :type s: List[str] :rtype: None Do not return anything, modify s in-place instead. \"\"\" # Logic is same as the one in iterative solution if not len(s): return [] self.reverseStringHelper(0, len(s) - 1, s) def reverseStringHelper(self, left, right, s): if left \u003e= right: return s[left], s[right] = s[right], s[left] self.reverseStringHelper(left + 1, right - 1, s) ","date":"2019-07-23","objectID":"/posts/leetcode/reverse-string/:0:1","tags":["Reverse String","Two Pointers","String","LeetCode","Algorithm"],"title":"[LeetCode] 344. Reverse String","uri":"/posts/leetcode/reverse-string/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Permutations Link to original Problem on LeetCode Given a collection of distinct integers, return all possible permutations. Example: Input: [1,2,3] Output: [ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1] ] Company: Microsoft, Adobe, Google Solution (Backtracking and Recursion): Time Complexity: Space Complexity: class Solution(object): def permute(self, nums): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" result = [] self.permuteHelper(nums, [], result) return result def permuteHelper(self, nums, path, result): if not nums: result.append(path) for i in range(len(nums)): self.permuteHelper(nums[:i] + nums[i + 1:], path + [nums[i]], result) Explanation: Following is how the recursion is working: permuteHelper(nums = [1, 2, 3] , path = [] , result = [] )\r|____ permuteHelper(nums = [2, 3] , path = [1] , result = [] )\r| |___permuteHelper(nums = [3] , path = [1, 2] , result = [] )\r| | |___permuteHelper(nums = [] , path = [1, 2, 3] , result = [[1, 2, 3]] ) # added a new permutation to the result\r| |___permuteHelper(nums = [2] , path = [1, 3] , result = [[1, 2, 3]] )\r| |___permuteHelper(nums = [] , path = [1, 3, 2] , result = [[1, 2, 3], [1, 3, 2]] ) # added a new permutation to the result\r|____ permuteHelper(nums = [1, 3] , path = [2] , result = [[1, 2, 3], [1, 3, 2]] )\r| |___permuteHelper(nums = [3] , path = [2, 1] , result = [[1, 2, 3], [1, 3, 2]] )\r| | |___permuteHelper(nums = [] , path = [2, 1, 3] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3]] ) # added a new permutation to the result\r| |___permuteHelper(nums = [1] , path = [2, 3] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3]] )\r| |___permuteHelper(nums = [] , path = [2, 3, 1] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1]] ) # added a new permutation to the result\r|____ permuteHelper(nums = [1, 2] , path = [3] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1]] )\r|___permuteHelper(nums = [2] , path = [3, 1] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1]] )\r| |___permuteHelper(nums = [] , path = [3, 1, 2] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2]] ) # added a new permutation to the result\r|___permuteHelper(nums = [1] , path = [3, 2] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2]] )\r|___permuteHelper(nums = [] , path = [3, 2, 1] , result = [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]] ) # added a new permutation to the result ","date":"2019-07-23","objectID":"/posts/leetcode/permutations/:0:1","tags":["Permutations","Backtracking","Array","LeetCode","Algorithm"],"title":"[LeetCode] 46. Permutations","uri":"/posts/leetcode/permutations/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"3Sum Link to original Problem on LeetCode Given an array nums of n integers, are there elements a, b, c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. Note: The solution set must not contain duplicate triplets. Example: Given array nums = [-1, 0, 1, 2, -1, -4], A solution set is: [ [-1, 0, 1], [-1, -1, 2] ] Company: Facebook, Amazon, Microsoft Solution: class Solution(object): def threeSum(self, nums): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" # Runtime: 540 ms # Memory Usage: 15.1 MB # Maintain a list to store the result result = [] # Sort the array so that the searching is easier nums.sort() length = len(nums) # Idea is that we will take a element # and then take the two pointer to the next element and the last element # Depending on the sum of the sum the two elements # compare to the element we are currently at, # we will either move the start pointer or the end pointer for i in range(length - 2): # Since we have sorted the list we know that if we encounter # any positive element all the element following it will be positive # sum of 3 positive element can never be 0, so break from the loop if that happens if nums[i] \u003e 0: break # if current number and previous number is same then continue # Since we don't want duplicate triplet if i\u003e 0 and nums[i] == nums[i-1]: continue # Setting two pointers start, end = i + 1, length - 1 # Keep doing this untill start and ends points to the same element while start \u003c end: sum = nums[start] + nums[end] + nums[i] # If sum of three is greater than 0 # means we need to decrese the sum # so decrease end pointer if sum \u003e 0: end -= 1 # If sum of three is less than 0 # means we need to increase the sum # so increase start pointer elif sum \u003c 0: start += 1 # Else we have found a triplet else: # Add triplet to the result result.append([nums[i], nums[start], nums[end]]) # These two loops are used to take care of the duplicate triplets while start \u003c end and nums[start] == nums[start+1]: start += 1 while start \u003c end and nums[end] == nums[end-1]: end -= 1 start += 1 end -= 1 return result ","date":"2019-06-17","objectID":"/posts/leetcode/3sum/:0:1","tags":["3Sum","Two Pointers","Array","LeetCode","Algorithm"],"title":"[LeetCode] 15. 3Sum","uri":"/posts/leetcode/3sum/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Missing Number Link to original Problem on LeetCode Given an array containing n distinct numbers taken from 0, 1, 2, …, n, find the one that is missing from the array. Example 1: Input: [3,0,1] Output: 2 Example 2: Input: [9,6,4,2,3,5,7,0,1] Output: 8 Note: Your algorithm should run in linear runtime complexity. Could you implement it using only constant extra space complexity? Company: Amazon, Google Solution 1 (Using XOR): class Solution(object): def missingNumber(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # We know XOR of same number is 0 # We also know that len(nums) is present in the list # iff it is not the number which is absent # So we keep len(nums) in result result = len(nums) # Then we loop through each number # and XOR the result with index and number for i, num in enumerate(nums): result ^= i result ^= num # By the end the absent number will be present in result return result Solution 2 (Using Sum): class Solution(object): def missingNumber(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # We can use the same code we used when doing the problem with XOR # We just need to replace the XOR symbols with - and plus # We know inside loop if we add index with sum, # we will end us getting sum from 0 to n-1 # So initially we keep n in sum sum = len(nums) for i, num in enumerate(nums): # Inside loop we are subtracting each number we saw # and adding the index sum -= num sum += i # Finally the sum will contain the number with is absent return sum # This can also be done in one line (Using Python 2): # return sum(range(len(nums) + 1)) - sum(nums) This can also be solved using Binary Search but then we will have a time complexity of O(n log n). ","date":"2019-06-17","objectID":"/posts/leetcode/missing-number/:0:1","tags":["Missing Number","Math","Array","LeetCode","Algorithm"],"title":"[LeetCode] 268. Missing Number","uri":"/posts/leetcode/missing-number/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Maximum Subarray Link to original Problem on LeetCode Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Example: Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Follow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle. Company: Facebook, Paypal, Yahoo, Microsoft, LinkedIn, Amazon, Goldman Sachs Solution 1 (Kadane’s Algorithm): class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # Algorithm used: Kadane’s Algorithm # Take two variables # One will keep track of the current max sum # Other one will keep track of the global max sum # Pointing at an element there can be two options # i. Either that element combined with the previous elements is the max sum # ii. Or the element itself is greater than the previous sum # Make the first element the maxCurrent as well as maxGlobal maxCurrent, maxGlobal = nums[0], nums[0] # Loop starting from the second element and make a check for num in nums[1:]: # compare between current element and current + previous maxCurrent maxCurrent = max(num, maxCurrent + num) # If maxCurrent \u003e maxGlobal then change maxGlobal # This is require in case say the current element is negative # and previous maxCurrent is positive # Then value of maxCurrent will be num + maxCurrent # and obviously it will be less then maxGlobal if maxCurrent \u003e maxGlobal: maxGlobal = maxCurrent return maxGlobal Solution 2 (Dynamic Programming): class Solution(object): def maxSubArray(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" # Bottom-Up-Approach # Make a copy of the array # We can also use the array itself, # but for the sake of dynamic programming I am creating a cache cache = nums[:] # Loop starting from second element for i in range(1, len(cache)): # If the previous element is negative or zero keep the number as it is # Else add the previous cummulative sum if cache[i - 1] \u003e 0: cache[i] = cache[i - 1] + cache[i] # Return max of the cache return max(cache) ","date":"2019-06-17","objectID":"/posts/leetcode/maximum-subarray/:0:1","tags":["Maximum Subarray","Dynamic Programming","Array","LeetCode","Algorithm"],"title":"[LeetCode] 53. Maximum Subarray","uri":"/posts/leetcode/maximum-subarray/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Merge Sorted Array Link to original Problem on LeetCode Given two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array. Note: The number of elements initialized in nums1 and nums2 are m and n respectively. You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2. Example: Input: nums1 = [1,2,3,0,0,0], m = 3 nums2 = [2,5,6], n = 3 Output: [1,2,2,3,5,6] Company: Adobe, Expedia, Microsoft Solution: class Solution(object): def merge(self, nums1, m, nums2, n): \"\"\" :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: None Do not return anything, modify nums1 in-place instead. \"\"\" # We can start comparing from the end of each array # Note for 1st array end is specified by m # We can swap after comparing # The largest elemtn will go to the end of 1st array # Which is actually a 0 while m \u003e 0 and n \u003e 0: if nums2[n - 1] \u003e= nums1[m - 1]: nums1[m + n - 1] = nums2[n - 1] n -= 1 else: nums1[m + n - 1] = nums1[m - 1] m -= 1 # If the first array is completed traversing before second # Put all the remaining element of second element inside 1st array # while n \u003e 0: # nums1[n - 1] = nums2[n - 1] # n -= 1 if n \u003e 0: nums1[:n] = nums2[:n] ","date":"2019-06-17","objectID":"/posts/leetcode/merge-sorted-array/:0:1","tags":["Merge Sorted Array","Two Pointers","Array","LeetCode","Algorithm"],"title":"[LeetCode] 88. Merge Sorted Array","uri":"/posts/leetcode/merge-sorted-array/"},{"categories":["Programming","Algorithms","LeetCode"],"content":"Sort Array By Parity Link to original Problem on LeetCode Given an array A of non-negative integers, return an array consisting of all the even elements of A, followed by all the odd elements of A. You may return any answer array that satisfies this condition. Example 1: Input: [3,1,2,4] Output: [2,4,3,1] The outputs [4,2,3,1], [2,4,1,3], and [4,2,1,3] would also be accepted. Note: 1 \u003c= A.length \u003c= 5000 0 \u003c= A[i] \u003c= 5000 Company: Amazon, Google, Microsoft, Adobe, Yahoo Solution: class Solution(object): def sortArrayByParity(self, A): \"\"\" :type A: List[int] :rtype: List[int] \"\"\" # Keep a pointer to the index with which the even number can be swap # For start it will be 0, since we want even numbers first index = 0 for i, num in enumerate(A): # If we get a even number swap it with the value in index position # Then increament the index to point to the next index # which can be used for swap if num % 2 == 0: A[i], A[index] = A[index], A[i] index += 1 # Now after every element is swaped, we can return the array return A ","date":"2019-06-17","objectID":"/posts/leetcode/sort-array-by-parity/:0:1","tags":["Sort Array by Parity","Two Pointers","Array","LeetCode","Algorithm"],"title":"[LeetCode] 905. Sort Array By Parity","uri":"/posts/leetcode/sort-array-by-parity/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Binary Tree Preorder Traversal Link to original Problem on LeetCode Given a binary tree, return the preorder traversal of its nodes’ values. Example: Output: [1, 3, 2] Follow up: Recursive solution is trivial, could you do it iteratively? Company: Microsoft, Amazon Recursive Solution: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def preorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" # Runtime: 16 ms # Memory Usage: 11.8 MB # Normal recursion solution returnList = [] self.doPreOrderTraversal(root, returnList) return returnList def doPreOrderTraversal(self, root, returnList): if not root: return returnList.append(root.val) self.doPreOrderTraversal(root.left, returnList) self.doPreOrderTraversal(root.right, returnList) Iterative Solution: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def preorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" # Runtime: 16 ms # Memory Usage: 11.6 MB # The idea is to maintain a stack # Keep the root in the stack # While the stack is not empty add the top of the stack to returnList # Since we need left element first we will push it to the stack after right element # Once the stack is empty we are done returnList = [] stack = [] stack.append(root) while stack: toTraverse = stack.pop() if toTraverse: returnList.append(toTraverse.val) stack.append(toTraverse.right) stack.append(toTraverse.left) return returnList ","date":"2019-05-15","objectID":"/posts/leetcode/binary-tree-preorder-traversal/:0:1","tags":["Algorithms","Binary Tree","Preorder Traversal","Programming"],"title":"[LeetCode] 144. Binary Tree Preorder Traversal","uri":"/posts/leetcode/binary-tree-preorder-traversal/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Binary Tree Postorder Traversal Link to original Problem on LeetCode Given a binary tree, return the postorder traversal of its nodes’ values. Example: Output: [3,2,1] Follow up: Recursive solution is trivial, could you do it iteratively? Company: Microsoft, Amazon Recursive Solution: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def postorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" # Runtime: 16 ms # Memory Usage: 11.9 MB # Normal recursion solution returnList = [] self.doPostOrderTraversal(root, returnList) return returnList def doPostOrderTraversal(self, root, returnList): if not root: return self.doPostOrderTraversal(root.left, returnList) self.doPostOrderTraversal(root.right, returnList) returnList.append(root.val) Iterative Solution: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def postorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" # Runtime: 8 ms # Memory Usage: 11.8 MB # The idea here is to maintain 2 stack. # One will be use to traverse the tree and keep track of left and right child # In the second stack we will keep on pushing those node whose child we have visited # If tree is empty then return empty list if root == None: return [] # Initialize two empty list. # These list we can use to implement 2 stack firstStack, secondStack = [], [] # Add root to the 1st Stack firstStack.append(root) returnList = [] # While first stack is not empty, add the top of 1st stack to 2nd stack # Add left and right of the pushed node to 1st stack while firstStack: moveToSecond = firstStack.pop() secondStack.append(moveToSecond) if moveToSecond.left: firstStack.append(moveToSecond.left) if moveToSecond.right: firstStack.append(moveToSecond.right) # While second stack is not empty then push the top node to returnList while secondStack: returnList.append(secondStack.pop().val) return returnList ","date":"2019-05-15","objectID":"/posts/leetcode/binary-tree-postorder-traversal/:0:1","tags":["Algorithms","Binary Tree","Postorder Traversal","Programming"],"title":"[LeetCode] 145. Binary Tree Postorder Traversal","uri":"/posts/leetcode/binary-tree-postorder-traversal/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Binary Tree Inorder Traversal Link to original Problem on LeetCode Given a binary tree, return the inorder traversal of its nodes’ values. Example: Output: [1, 2, 3] Follow up: Recursive solution is trivial, could you do it iteratively? Company: Microsoft, Amazon Recursive Solution: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def inorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" # Runtime: 12 ms # Memory Usage: 11.8 MB # Normal recursion solution returnList = [] self.doInOrderTraversal(root, returnList) return returnList def doInOrderTraversal(self, root, returnList): if not root: return self.doInOrderTraversal(root.left, returnList) returnList.append(root.val) self.doInOrderTraversal(root.right, returnList) Iterative Solution: # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def inorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" # Runtime: 12 ms # Memory Usage: 11.6 MB # Here the idea is to push every left node to the stack # If there is no left node available pop and add top of stack to returnList, # then go to right and from there also go to very left # Once both stack is empty and root is None, break returnList = [] stack = [] while True: # Go as left as possible and with each node travesed add it to stack if root: stack.append(root) root = root.left # If there is no left the pop from stack # Add it to the returnList and go to the right # If right is not there again check whether stack is empty elif stack: root = stack.pop() returnList.append(root.val) root = root.right # If root is None and stack is empty then break else: break return returnList ","date":"2019-05-15","objectID":"/posts/leetcode/binary-tree-inorder-traversal/:0:1","tags":["Algorithms","Binary Tree","Inorder Traversal","Programming"],"title":"[LeetCode] 94. Binary Tree Inorder Traversal","uri":"/posts/leetcode/binary-tree-inorder-traversal/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Two Sum Link to original Problem on LeetCode Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9, return [0, 1]. Company: Facebook, Amazon, Google Solution: class Solution(object): def twoSum(self, nums, target): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" # We can create a hashmap with key-value pair as number and it's index hashTable = {} for i, num in enumerate(nums): # We will search whether we have already encounter the target - current num value if target - num in hashTable: # If yes then we will return the index of # that encountered value and current value's index return [hashTable[target - num], i] break # In each iteration we are storing value and it's index in the hashmap hashTable[num] = i ","date":"2019-05-14","objectID":"/posts/leetcode/two-sum/:0:1","tags":["Algorithms","Two Sum","Arrays","Programming"],"title":"[LeetCode] 1. Two Sum","uri":"/posts/leetcode/two-sum/"},{"categories":["Algorithms","Programming","LeetCode"],"content":"Longest Substring Without Repeating Characters Link to original Problem on LeetCode Given a string, find the length of the longest substring without repeating characters. Example 1: Input: “abcabcbb” Output: 3 Explanation: The answer is “abc”, with the length of 3. Example 2: Input: “bbbbb” Output: 1 Explanation: The answer is “b”, with the length of 1. Example 3: Input: “pwwkew” Output: 3 Explanation: The answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring. Company: Amazon Solution: class Solution(object): def lengthOfLongestSubstring(self, s): \"\"\" :type s: str :rtype: int \"\"\" # In this solution we are keeping a sliding window # As we move through the string we are keeping track of what character we have seen using hashmap # We have two pointers. # One to keep track of the index of current variable scanned # And other to move the sliding window in case the window have repeating characters secondPointer, maxLength = 0, 0 charIndexDict = {} for firstPointer, char in enumerate(s): # In case the window have repeating character # Move the second pointer to the next index # of the previous occurence of currenty scanned character if char in charIndexDict: secondPointer = max(secondPointer, charIndexDict[char] + 1) # Add the index of the character in the hashmap charIndexDict[char] = firstPointer # Assign maxLength to new window size only if it has increased maxLength = max(maxLength, firstPointer - secondPointer + 1) return maxLength ","date":"2019-05-14","objectID":"/posts/leetcode/longest-substring-without-repeating-characters/:0:1","tags":["Algorithms","Substring","Strings","Programming"],"title":"[LeetCode] 3. Longest Substring Without Repeating Characters","uri":"/posts/leetcode/longest-substring-without-repeating-characters/"},{"categories":["Algorithms","Programming","Interview Preparation"],"content":"FizzBuzz Link to original Problem on LeetCode Write a program that outputs the string representation of numbers from 1 to n. But for multiples of three, it should output “Fizz” instead of the number and for the multiples of five output “Buzz”. For numbers which are multiples of both three and five output “FizzBuzz”. Example: n = 15, Return: [ “1”, “2”, “Fizz”, “4”, “Buzz”, “Fizz”, “7”, “8”, “Fizz”, “Buzz”, “11”, “Fizz”, “13”, “14”, “FizzBuzz” ] My Solution: class Solution(object): def fizzBuzz(self, n): \"\"\" :type n: int :rtype: List[str] \"\"\" return ['Fizz' * (not i % 3) + 'Buzz' * (not i % 5) or str(i) for i in range(1, n + 1)] Explanation: In python, if you multiply any string to an integer you will get the same string repeated the same number of you multiplied it. \u003e\u003e\u003e'Fizz' * 3 'FizzFizzFizz' Now, in arithmetic, python treats booleans as integers. True is treated as 1 and False is treated as 0. In the code not i % 3 will return a boolean which then can be multiplied with Fizz. Depending on whether the number is divisible by 3 or not we will get the output. The same goes for Buzz. If the number is divisible by both 3 and 5 then we are using string concatenation to get FizzBuzz. An empty string is considered False. That’s why we are using or in between to make sure if the number is not divisible by either 3 or 5 then use the string typecast version of the number. We have use string comprehension to do all this in one line. ","date":"2019-05-13","objectID":"/posts/leetcode/coding-interview-warmup-1/:0:1","tags":["Coding Interview","Warmup","Algorithms","Programming"],"title":"Coding Interview: Warm Up","uri":"/posts/leetcode/coding-interview-warmup-1/"},{"categories":["Algorithms","Programming","Interview Preparation"],"content":"Subarray Sum Equals K Link to original Problem on LeetCode Given an array of integers and an integer k, you need to find the total number of continuous subarrays whose sum equals to k. Example 1: Input:nums = [1,1,1], k = 2 Output: 2 Note: The length of the array is in the range [1, 20,000]. The range of numbers in the array is [-1000, 1000] and the range of the integer k is [-1e7, 1e7]. My Solution: class Solution(object): def subarraySum(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: int \"\"\" sumNums, countDict, count = 0, {}, 0 for num in nums: sumNums += num if sumNums == k: count += 1 if sumNums - k in countDict: count += countDict[sumNums - k] countDict[sumNums] = countDict.get(sumNums, 0) + 1 return count Explanation: We are maintaining a variable (sumNums) with a cumulative sum up to the index we are looking (up to i). Also, a dictionary is maintained with cumulative sum as key and their count as value. As an example we can take an array : [2, 1, 3, 6, -3, 3, 5, 1] The cumulative sum for this array will be: [2, 3, 6, 12, 9, 12, 17, 18] The dictionary will be: {2: 1, 3: 1, 6: 1, 12: 2, 9: 1, 17: 1, 18: 1} Let k = 6 Now if in any point we see the number k as cumulative sum, increase the count by 1 (at index 2). Also, we increase the count of each time we found a key equals to cumulative sum - k If we see index 7, it has a cumulative sum = 18. So cummulative sum - k =\u003e 18 - 6 = 12. We have 12 two time in the dictionary (at index 3 and 5). So if we take the subarray after those index up to the index 7 (from index 4 to 7 and index 6 to 7 ie. subarray [-3, 3, 5, 1] and subarray [5, 1]) then we will have a sum of 6. ","date":"2019-05-13","objectID":"/posts/leetcode/coding-interview-warmup-1/:0:2","tags":["Coding Interview","Warmup","Algorithms","Programming"],"title":"Coding Interview: Warm Up","uri":"/posts/leetcode/coding-interview-warmup-1/"},{"categories":["Algorithms","Programming","Interview Preparation"],"content":"Valid Anagram Link to original Problem on LeetCode Given two strings s and t, write a function to determine if t is an anagram of s. Example 1: Input: s = “anagram”, t = “nagaram” Output: true Example 2: Input: s = “rat”, t = “car” Output: false Note: You may assume the string contains only lowercase alphabets. My Solution: class Solution(object): def isAnagram(self, s, t): \"\"\" :type s: str :type t: str :rtype: bool \"\"\" if len(s) != len(t): return False sDict, tDict = {}, {} for char in s: sDict[char] = sDict.get(char, 0) + 1 for char in t: tDict[char] = tDict.get(char, 0) + 1 return sDict == tDict Explanation: Maintain a dictionary with the count of each letter encountered till now. At the end just return whether the two dictionaries are equal or not. ","date":"2019-05-13","objectID":"/posts/leetcode/coding-interview-warmup-1/:0:3","tags":["Coding Interview","Warmup","Algorithms","Programming"],"title":"Coding Interview: Warm Up","uri":"/posts/leetcode/coding-interview-warmup-1/"},{"categories":["Algorithms","Programming","Interview Preparation"],"content":"Rotate Array Link to original Problem on LeetCode Given an array, rotate the array to the right by k steps, where k is non-negative. Example 1: Input: [1,2,3,4,5,6,7] and k = 3 Output: [5,6,7,1,2,3,4] Explanation: rotate 1 steps to the right: [7,1,2,3,4,5,6] rotate 2 steps to the right: [6,7,1,2,3,4,5] rotate 3 steps to the right: [5,6,7,1,2,3,4] Example 2: Input: [-1,-100,3,99] and k = 2 Output: [3,99,-1,-100] Explanation: rotate 1 steps to the right: [99,-1,-100,3] rotate 2 steps to the right: [3,99,-1,-100] My Solution: class Solution(object): def rotate(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: None Do not return anything, modify nums in-place instead. \"\"\" nums[:] = nums[len(nums) - k:] + nums[:len(nums) - k] Explanation: We are using python’s list slicing feature. Here I am slicing the list first from the element which should come first to the very end and then slice it again till the element which should be present in the end of new list. Then just add this two sliced list and assign to same list and return. ","date":"2019-05-13","objectID":"/posts/leetcode/coding-interview-warmup-1/:0:4","tags":["Coding Interview","Warmup","Algorithms","Programming"],"title":"Coding Interview: Warm Up","uri":"/posts/leetcode/coding-interview-warmup-1/"},{"categories":["Programming","Machine Learning","Python"],"content":" TensorFlow Basic I made this post for revision purpose. This post contains most of the tensorflow basics and how does they work in a sense. Most of the code is beginner friendly. There is no need for pre-requisite programming knowledge of tensorflow in any sense to go through this notebook, but you should have a basic understanding of Python and how array works in general (also if you have a knowledge of AI, that would be great). The unit of data in TensorFlow s called a tensor. A tensor is basically speaking is a multidimensional arrays (though it is not the case, but still if you look from the perspective of beginner it looks like a multidimensional array). Each tensor have something called rank and it is its number of dimensions (Example for this is given in next part). So first of all at the very beginning we need to import tensorflow to work with the Classes, methods and symbols associated with it. For this below is the code(which is typical code to import a library in Python). import tensorflow as tf Now we can use all TensorFlow’s classes, methods, and symbols ","date":"2018-01-26","objectID":"/posts/basics_learning/tensorflow-basics/:0:0","tags":["TensorFlow","Basics","Machine Learning","Python","Tutorial"],"title":"TensorFlow Basic","uri":"/posts/basics_learning/tensorflow-basics/"},{"categories":["Programming","Machine Learning","Python"],"content":"Constants Let’s create a very basic constant. For this TensorFlow used what it calls is a tensor object. tf.constant('Hello World') \u003ctf.Tensor 'Const:0' shape=() dtype=string\u003e This is a fundamental String constant. We can also save this as a variable. hello = tf.constant('Hello World') Let’s see it’s type: type(hello) tensorflow.python.framework.ops.Tensor See, at the very end, we have ‘Tensor’, indicating that it’s an object of Tensor. We can also create integer constant or float constant. a = tf.constant(2) # we can explicitly pass in the data type of the constant x = tf.constant(3.5, dtype=tf.float32) # or we can also tf.float32 implicitly y = tf.constant(5.0) Let’s again check the type. type(a) tensorflow.python.framework.ops.Tensor type(x) tensorflow.python.framework.ops.Tensor type(y) tensorflow.python.framework.ops.Tensor Do you want to print the constants? Let’s try. print(a, x, y) Tensor(\"Const_2:0\", shape=(), dtype=int32) Tensor(\"Const_3:0\", shape=(), dtype=float32) Tensor(\"Const_4:0\", shape=(), dtype=float32) What just happened? Notice this print does not prints: 2 3.5 5.0 The thing is that each of the object(sometime called nodes) will be printed if evaluated inside a so-called session. A session is something that encapsulate the state and control the TensorFlow runtime. In other word (a bit technical) a session encapsulate the environment in which operation objects are executed. Tensor objects are evaluated in those operation objects. ","date":"2018-01-26","objectID":"/posts/basics_learning/tensorflow-basics/:0:1","tags":["TensorFlow","Basics","Machine Learning","Python","Tutorial"],"title":"TensorFlow Basic","uri":"/posts/basics_learning/tensorflow-basics/"},{"categories":["Programming","Machine Learning","Python"],"content":"Session To create a session, we need to use a class Session. We can do this in this way. sess = tf.Session() Now to evaluate nodes inside a session, we must run something called a computational graph. First, let’s talk about the computational graph in general. The computational graph is basically everywhere in computer science. Think of this statement: e=(a+b)∗(b+1) Here we can see that we have 3 operations, 2 addition and 1 multiplication and the computational graph can be visualized in this manner: For more please go through the Computational graph part in this article Now if we run a computational graph in a session we will have the desired output. For this, we can use run() method. print(sess.run([a, x, y])) [2, 3.5, 5.0] If we check the type of tensor object inside sess.run() we can find that it’s now being evaluated as numpy array. type(sess.run(a)) numpy.int32 type(sess.run(x)) numpy.float32 Let’s do the same thing with string constant we created earlier. sess.run(hello) b'Hello World' In the above output ‘b’ represents that the string is bytestring. We can check this: type(sess.run(hello)) bytes Some extra note: TensorFlow converts str to bytes in most places, including sess.run(). When using print with the string constant we get: print(sess.run(hello)) b'Hello World' which is not the desired output. We are getting ‘b’ in front of the string(this is because it’s type is byte not str). To get the actual string you can use decode() method like this: print(sess.run(hello).decode()) Hello World ","date":"2018-01-26","objectID":"/posts/basics_learning/tensorflow-basics/:0:2","tags":["TensorFlow","Basics","Machine Learning","Python","Tutorial"],"title":"TensorFlow Basic","uri":"/posts/basics_learning/tensorflow-basics/"},{"categories":["Programming","Machine Learning","Python"],"content":"Operation Let’s now move to Operations. We can do multiple operations which includes addition, subtraction, multiplication, division, etc. x = tf.constant(4) y = tf.constant(5) with tf.Session() as sess: print('Operations on', sess.run(x), 'and', sess.run(y), ':') print('Addition:', sess.run(x+y)) print('Subtraction:', sess.run(x-y)) print('Multiplication:', sess.run(x*y)) print('Division:', sess.run(x/y)) Operations on 4 and 5 : Addition: 9 Subtraction: -1 Multiplication: 20 Division: 0.8 ","date":"2018-01-26","objectID":"/posts/basics_learning/tensorflow-basics/:0:3","tags":["TensorFlow","Basics","Machine Learning","Python","Tutorial"],"title":"TensorFlow Basic","uri":"/posts/basics_learning/tensorflow-basics/"},{"categories":["Programming","Machine Learning","Python"],"content":"Placeholder It’s not like we are always going to work with constant. TensorFlow has another type of object called placeholder which can accept a value, and after that, we can do an operation on that value. To create placeholder we will use tf.placeholder(). Inside parenthesis, you can put the datatype(or object type for tensor) you want the placeholder to hold. x = tf.placeholder(tf.int32) y = tf.placeholder(tf.int32) Other than this there are a bunch of other placeholders. If you are using iPython, then you can explore those by typing ’tf.int’ or ’tf.float’ followed by tab. x \u003ctf.Tensor 'Placeholder:0' shape=\u003cunknown\u003e dtype=int32\u003e y \u003ctf.Tensor 'Placeholder_1:0' shape=\u003cunknown\u003e dtype=int32\u003e We can see that these are the placeholder and the initial shape is unknow as it does not hold anything initially. type(x) tensorflow.python.framework.ops.Tensor type(y) tensorflow.python.framework.ops.Tensor We can also define operation using tensorflow. Below we have 4 operation tensorflow provides such as tf.add, tf.subtract, tf.divide, tf.multiply. Other then these we have a bunch of other operation (which obviously includes matrix operation) inside TensorFlow. If you have worked with lambda before then, this might seem a bit similar to you where we are defining two parameters first and then the operation to be performed on them. add = tf.add(x, y) sub = tf.subtract(x, y) mul = tf.multiply(x, y) div = tf.divide(x, y) To evaluate these operations inside run we can use something called a feed_dict argument. Basically the syntax is that the run method will accept an operation followed by the feed dictionary which tell on what value we have to do the operations. Below we have 4 dictionary containing different values for x and y (key-value pair format). d1 = {x:5, y:6} d2 = {x:2, y:8} d3 = {x:7, y:2} d4 = {x:9, y:5} Now we can do operation using the placeholder. with tf.Session() as sess: print('Operations with placeholders:') print('Addition:', sess.run(add, feed_dict=d1)) print('Subtraction:', sess.run(sub, feed_dict=d2)) print('Multiplication:', sess.run(mul, feed_dict=d3)) print('Division:', sess.run(div, feed_dict=d4)) Operations with placeholders: Addition: 11 Subtraction: -6 Multiplication: 14 Division: 1.8 Before ending this let’s look at how we are going to do operation on matrix in a very basic level. First we will have to import numpy to create array. Then we will do operation on that array. import numpy as np # Two array, one of dimension 2 by 3 and other of diension 3 by 2 a = np.array([[6.0, 6.0, 6.0], [2.0, 2.0, 2.0]]) b = np.array([[5.0, 10.0], [5.0, 10.0], [5.0, 10.0]]) a.shape (2, 3) b.shape (3, 2) # Creating TensorFlow constants mat1 = tf.constant(a) mat2 = tf.constant(b) # defining operation matrixMul = tf.matmul(mat1, mat2) # Creating session and running with tf.Session() as sess: print(sess.run(matrixMul)) [[ 90. 180.] [ 30. 60.]] If you want to explore more, you could (if you are using iPython or an IDE) type ’tf.mat’ followed by tab to explore other operation available for the matrix. Before ending there is one fascinating thing which you might need in future. For those working with neural network this feature of numpy is quite handy. When we are working with bias. Think of CNN where we have weights (W) and image matrix (X). We have a equation which looks something like this y = X.W + b. Here both result of X.W and b have different shape. Then also we can add them using something called broadcasting. Let’s look few examples p = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) q = np.array([5.0]) p * q array([[ 5., 10., 15.], [ 20., 25., 30.]]) Above 5.0 is multiplied with every line. This feature is quite handy when dealing with situations where the dimensions of parameters to performed operations on do not matches. Using the same feature in TensorFlow: x = np.array([[6.0], [6.0]]) mat3 = tf.constant(x) with tf.Session() as sess: print(sess.run(matrixMul + x)) [[ 96. 186.] [ 36. 66.]] These are some of the fundamental conc","date":"2018-01-26","objectID":"/posts/basics_learning/tensorflow-basics/:0:4","tags":["TensorFlow","Basics","Machine Learning","Python","Tutorial"],"title":"TensorFlow Basic","uri":"/posts/basics_learning/tensorflow-basics/"},{"categories":["Programming","Web Development","API"],"content":" 1. What are the tools needed? 2. Basic setup 3. Setting up Tomcat Server and Postman 4. Structure 5. Book Model 6. Book Database 7. Book Services 8. Book Resource 9. GET 10. PUSH 11. PUT 12. DELETE 13. Query -Get Book(s) by title -Get Book(s) by subject ID Another thing we are going to use is called Postman. Postman is an HTTP Request composer. It helps you test your API in a very efficient way. You can download Postman from here. Fill in the following data Archetype Artifact Id: jersey-quickstart-webapp Archetype Version: 2.16 Alternatively you can add the following dependency after creating your Maven Project. \u003c!-- https://mvnrepository.com/artifact/org.glassfish.jersey.archetypes/jersey-quickstart-webapp --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.glassfish.jersey.archetypes\u003c/groupId\u003e \u003cartifactId\u003ejersey-quickstart-webapp\u003c/artifactId\u003e \u003cversion\u003e2.16\u003c/version\u003e \u003c/dependency\u003e Now next you need to type your project detail. Mine is: Group Id: org.debakar.bakadigest Artifact Id: bookDatabase This will create a package org.debakar.bakadigest.bookDatabase where your resource files are stored. The very first thing you need to do is open up your project and double-click pom.xml. Now head to pom.xml tab and uncomment moxy dependency. This will let you produce JSON data. This is the same place where you can add jersey dependency after creating a Maven Project using maven archetype. \u003cdependency\u003e \u003cgroupId\u003eorg.glassfish.jersey.media\u003c/groupId\u003e \u003cartifactId\u003ejersey-media-moxy\u003c/artifactId\u003e \u003c/dependency\u003e And for Postman does need any particular step to install. You just need to install it basically as you install other application or add the chrome app available if you are using chrome. Here’s the project structure: NOTE: This is not a great way when you do it for business purpose. ","date":"2017-09-27","objectID":"/posts/basics_learning/building-a-restful-api-using-jax-rs/:0:0","tags":["RESTful","API","JAX-RS","Java","Tutorial","Web Development"],"title":"Building a Restful API using JAX-RS","uri":"/posts/basics_learning/building-a-restful-api-using-jax-rs/"},{"categories":["Programming","C++"],"content":" So most of the students in Computer Science Department of a B.Tech college have written code in C++ at some point in their life. Let me begin with a small code snippet. #include\u003ciostream\u003e using namespace std; namespace abc { int a = 100; } int a = 200; int main() { int a = 300; cout \u003c\u003c \"abc::a = \" \u003c\u003c abc::a \u003c\u003c endl; cout \u003c\u003c \"::a = \" \u003c\u003c::a \u003c\u003c endl; cout \u003c\u003c \"a = \" \u003c\u003c a \u003c\u003c endl; return (0); } Running the code produce this output: abc::a = 100 ::a = 200 a = 300 If you look at the code you will see we always start with something like #include\u003ciostream\u003e, then add some more #include\u003c...\u003e (if needed). We usually call them header file. But if you have ever written code in C language then you might be curious why do we use a header file without .h at the end in C++ (and why not with .h like we used to do while writing code in C). The reason behind is rather simple but it has too many chain elements. The simple answer to this question is iostream is a part of C++ standard header while iostream.h is not. iostream.h is disapproved by some latest C++ compiler. To be more precise there is no mention of iostream.h at all in the current C++ standard (ISO/IEC DIS 14882). Again we can use headers like stdio.h in C++ without any problem. But that is not the convention. C++ is derived from C and hence we permitted to use the header we previously used in C. But in C++ application you shouldn’t use stdio.h, but you should use instead cstdio. You can use most of the C header in C++ but all you need to do is remove the .h part and add c before the header name. example: #include\u003ccstdio\u003e #include\u003ccstddef\u003e #include\u003ccstring\u003e #include\u003ccwchar\u003e Now coming to next part i.e. using namespace std. Why do we use it? Try omitting using namespace std and run the code. #include\u003ciostream\u003e //using namespace std; namespace abc { int a = 100; } int a = 200; int main() { int a = 300; cout \u003c\u003c \"abc::a = \" \u003c\u003c abc::a \u003c\u003c endl; cout \u003c\u003c \"::a = \" \u003c\u003c::a \u003c\u003c endl; cout \u003c\u003c \"a = \" \u003c\u003c a \u003c\u003c endl; return (0); } You will get an error which looks something like this: error: 'cout' was not declared in this scope When you use a C++ library without .h, such as #include\u003ciostream\u003e then it load all the symbols in iostream header file inside std namespace (We will discuss namespace in next part). NOTE: The std (abbrevate for standard) namespace is empty by default. Header symbols a loaded in std namespace when they are include without .h. If you omit using namespace std then, you will not be able to use the reference such as cout or cin or even endl. What is actually a namespace? Although this cannot be answered in a single blog post as namespace is a very advanced topic in C++. Though to be logical here, namespace is nothing but a container. It contains a set of symbols. When you use using namespace std it actually invokes this: namespace std { #include \u003ciostream.h\u003e }; #include\u003ciostream\u003e //using namespace std; namespace abc { int a = 100; } int a = 200; int main() { int a = 300; std::cout \u003c\u003c \"abc::a = \" \u003c\u003c abc::a \u003c\u003c std::endl; std::cout \u003c\u003c \"::a = \" \u003c\u003c::a \u003c\u003c std::endl; std::cout \u003c\u003c \"a = \" \u003c\u003c a \u003c\u003c std::endl; return (0); } So what using namespace std does so that we can use cout without std::? Basically, it loads all the symbols in C++ Standard Library and makes them global (To understand this you need to have a perfect knowledge of scope, or carry along this blog. I will try to make it more clear). So now let’s talk about what we learn at the beginning, that using namespace std will load all the standard library symbols as well as make them global. So we can use cout in place of std::cout as cout is now global and we haven’t redefined in inside main function (So only cout will work. No need of ::cout. Thought that will work too). So this is how all this works. I hope you like reading this blog. If you have any question then feel free to comment below. Also if you like, you can share it with your friends. ","date":"2017-08-20","objectID":"/posts/basics_learning/understanding-the-very-first-few-lines-of-basic-c++-code/:0:0","tags":["C++","Code","Basics","Tutorial","Understanding"],"title":"Understanding the very first few lines of basic C++ code","uri":"/posts/basics_learning/understanding-the-very-first-few-lines-of-basic-c++-code/"},{"categories":["Programming","Java"],"content":"Assignment 1 Write a Java program to print “Hello World”. Source code: public class HelloWorldnormal {\rpublic static void main(String[] args) {\rSystem.out.println(\"Hello World\");\r}\r} Output: $ javac HelloWorldnormal.java\r$ java HelloWorldnormal\rHello World Write a Java program to.pront “Hello World”, where ‘World’ will be taken from command line argument. Source code: public class HelloWorldCLA {\rpublic static void main(String[] args) {\rSystem.out.println(\"Hello \" + args[0]);\r}\r} Output: $ javac HelloWorldCLA.java\r$ java HelloWorldCLA World\rHello World Source code: public class SumOf2NumbersCLA {\rpublic static void main(String[] args) {\rSystem.out.println(\"Sum of \" + args[0] + \" and \" + args[1] + \" is \" + (Integer.parseInt(args[0]) + Integer.parseInt(args[1])));\r}\r} Output: $ javac SumOf2NumbersCLA.java\r$ java SumOf2NumbersCLA 7 8\rSum of 7 and 8 is 15 0 HELLO NAME1\r1 HELLO NAME2\r2 HELLO NAME3 Source code: public class Greeting {\rpublic static void main(String[] args) {\rfor (int i = 0; i \u003c args.length; i++) {\rSystem.out.println(i + \" HELLO \" + args[i]);\r}\r}\r} Output: $ javac Greeting.java\r$ java Greeting Debakar Baka Dr_NULL\r0 HELLO Debakar\r1 HELLO Baka\r2 HELLO Dr_NULL Source code: import java.util.Scanner;\rpublic class Swap2Numbers {\rpublic static void main(String[] args) {\rScanner input = new Scanner(System.in);\rSystem.out.print(\"\\nEnter two numbers: \");\rint a = input.nextInt(), b = input.nextInt();\rinput.close();\rSystem.out.println(\"\\nBefore Swap: a = \" + a + \", b = \" + b);\ra = a + b;\rb = a - b;\ra = a - b;\rSystem.out.println(\"\\nAfter Swap: a = \" + a + \", b = \" + b);\r}\r} Output: $ javac Swap2Numbers.java\r$ java Swap2Numbers\rEnter two numbers: 67 54\rBefore Swap: a = 67, b = 54\rAfter Swap: a = 54, b = 67 1 2 3 4 5\r2 3 4 5\r3 4 5\r4 5\r5 Source code: import java.util.Scanner;\rpublic class Pattern1 {\rpublic static void main(String[] args) {\rScanner input = new Scanner(System.in);\rSystem.out.print(\"\\nEnter number of line for the pattern: \");\rint n = input.nextInt();\rinput.close();\rfor (int i = 1; i \u003c= n; i++) {\rfor (int j = i; j \u003c= n; j++) {\rSystem.out.print(j + \" \");\r}\rSystem.out.println();\r}\r}\r} Output: $ javac Pattern1.java\r$ java Pattern1\rEnter number of line for the pattern: 5\r1 2 3 4 5\r2 3 4 5\r3 4 5\r4 5\r5 if marks \u003c= 40 - Fail\r41 to 50 - Average\r51 to 60 - Fair\r61 to 70 - Good\r71 to 80 - Very good\r81 to 90 - Excellent\r91 to 100 - Outstanding Source code: import java.util.Scanner;\rpublic class RemarkCard {\rpublic static void main(String[] args) {\rScanner input = new Scanner(System.in);\rSystem.out.print(\"\\nEnter the marks of the students: \");\rint marks = input.nextInt();\rinput.close();\rif (marks \u003c= 40)\rSystem.out.println(\"Fail.\");\relse if (marks \u003e= 41 \u0026\u0026 marks \u003c= 50)\rSystem.out.println(\"Average.\");\relse if (marks \u003e= 51 \u0026\u0026 marks \u003c= 60)\rSystem.out.println(\"Fair.\");\relse if (marks \u003e= 61 \u0026\u0026 marks \u003c= 70)\rSystem.out.println(\"Good.\");\relse if (marks \u003e= 71 \u0026\u0026 marks \u003c= 80)\rSystem.out.println(\"Very Good.\");\relse if (marks \u003e= 81 \u0026\u0026 marks \u003c= 90)\rSystem.out.println(\"Excellent.\");\relse\rSystem.out.println(\"Outstanding.\");\r}\r} Output: $ javac RemarkCard.java\r$ java RemarkCard\rEnter the marks of the students: 95\rOutstanding. Write a Java program to print this kind of pattern. *\r* *\r* * *\r* * * *\r* * * * * Source code: import java.util.Scanner;\rpublic class Pattern2 {\rpublic static void main(String[] args) {\rScanner input = new Scanner(System.in);\rSystem.out.println(\"\\nEnter number of line for the pattern: \");\rint n = input.nextInt();\rinput.close();\rfor (int i = 0; i \u003c n; i++) {\rfor (int j = i; j \u003c n - 1; j++) {\rSystem.out.print(\" \");\r}\rfor (int j = 0; j \u003c= i; j++) {\rSystem.out.print(\"* \");\r}\rSystem.out.println();\r}\r}\r} Output: $ javac Pattern2.java\r$ java Pattern2\rEnter number of line for the pattern: 5\r*\r* *\r* * *\r* * * *\r* * * * * ","date":"2017-08-06","objectID":"/posts/basics_learning/java-assignment-1/:1:0","tags":["Java","Assignment","Programming","Tutorial"],"title":"[OOP (CS 594D)] Assignment 1","uri":"/posts/basics_learning/java-assignment-1/"},{"categories":["Database Management","Programming"],"content":" This time I am a bit late. Actually, my college started so it took me some time to write-up my next blog post. Anyways, today we are going to have a look at PL/SQL Cursor, Procedure, Function, and Trigger. This will complete the very basics of PL/SQL. So what is cursor? Cursor is a pointer to a memory area called context area(Actually it is a pointer to a row). Whenever you use SELECT or any DML (INSERT, DELETE, UPDATE or MERGE) then cursor holds the rows (one or more) returned by a SQL statement. There are two types of cursors: Implicit cursors Explicit cursors Implicit cursors: It is automatically created by the Oracle server every time an SQL DML statement is executed and the user cannot control the behavior of these cursors. Example: SET SERVEROUTPUT ON; BEGIN UPDATE PRODUCT SET P_NAME='WAIWAI' WHERE P_NAME LIKE ('FULL%'); DBMS_OUTPUT.PUT_LINE(SQL%ROWCOUNT); END; Here the output will be number of rows affected by the UPDATE (DML) statement. Explicit cursors: Explicit cursors, unlike implicit cursor, are user defined cursors. The user has to create these cursors for any statement which basically returns one or more row of data. Here the user has full control of the cursor. It is worth noting that an explicit cursor has to be named in the declaration section of the PL/SQL block. Here’s an example: SET SERVEROUTPUT ON; Declare CURSOR CURS_product IS SELECT P_ID,P_NAME, P_QTY FROM PRODUCT; V_RECORD CURS_product%ROWTYPE; BEGIN OPEN CURS_product; LOOP FETCH CURS_product INTO V_RECORD; EXIT WHEN CURS_product%NOTFOUND; DBMS_OUTPUT.PUT_LINE('PRODUCT NAME: ' || V_RECORD.P_NAME || chr(9) || ', PRODUCT ID: ' || V_RECORD.P_ID || ', PRODUCT QUANTITY: ' || V_RECORD.P_QTY); END LOOP; CLOSE CURS_product; END; So now you know what is a cursor and how to use implicit and explicit cursor. I would recommend you to look to the over the internet for more information on cursor. Now lets talk about procedure and function Just like other languages here in PL/SQL too a function is a set of statements which finally return something to the caller. Similarly a procedure too is a set of statement but it does not return anything. These two can save us time as we don’t have to rewrite the same long code again and again. Let me provide you the syntax for both first. --Syntax for function CREATE [OR REPLACE] FUNCTION function_name (Parameter 1, Parameter 2…) RETURN datatype IS Declare variable, constant etc. here. BEGIN Executable Statements Return (Return Value); END; / --Syntax for procedure CREATE [OR REPLACE] PROCEDURE procedure_name [ (parameter [,parameter]) ] IS [declaration_section] BEGIN executable_section [EXCEPTION exception_section] END [procedure_name]; Here’s an example of update product quantity in product tableeverytime someone purchased something i.e. data/row is added to the sales table: create or replace PROCEDURE PROD_PRODUCTS (PRO_ID NUMBER, PRO_QTY NUMBER , CUS_ID NUMBER, EMP_ID NUMBER, PRO_DATE DATE) AS BEGIN INSERT INTO SALES(C_ID, E_ID, P_ID, PUR_QTY, PUR_DATE) VALUES(CUS_ID , EMP_ID, PRO_ID, PRO_QTY, PRO_DATE); UPDATE PRODUCT SET P_QTY = ((SELECT P_QTY FROM PRODUCT WHERE P_ID = PRO_ID) - PRO_QTY)WHERE P_ID = PRO_ID; END; / EXEC PROD_PRODUCTS ( 10001, 2, 10000001, 100001, TO_DATE('18-JUL-2017', 'DD-MON-YYYY')) Similarly we can have a function to return number of bills count in a particular day from sales table. create FUNCTION FUNC_BILLS (P_DATE DATE) RETURN NUMBER AS V_BILLCOUNT NUMBER; BEGIN SELECT COUNT(*) INTO V_BILLCOUNT FROM SALES WHERE PUR_DATE = P_DATE; RETURN V_BILLCOUNT; END; / SET SERVEROUTPUT ON; DECLARE V_DATE DATE := TO_DATE('\u0026E_DATE', 'DD-MON-YYYY'); V_NUM NUMBER; BEGIN V_NUM := FUNC_BILLS(V_DATE); DBMS_OUTPUT.PUT_LINE(V_NUM); END; So now we know about cursor, procedure, and function. So that left us with our last topic, trigger. What are trigger?' Triggers are named PL/SQL blocks which automatically execute or fire whenever some event occur. In sence of PL/SQL lets say we want to call a function whenever a data is i","date":"2017-07-27","objectID":"/posts/oracle_dbms/oracle-dbms-day-5/:0:0","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 5"],"title":"[Training] DBMS with Oracle Day 5","uri":"/posts/oracle_dbms/oracle-dbms-day-5/"},{"categories":["Database Management","Programming"],"content":" If you are reading this then I want to notify you that this is the 3rd part of the series of blog post I am writing up about basically DBMS with Oracle. If you haven’t yet read my previous post then feel free to go to this link for part 1 and this link for part 2. Now today we are going to look at something called PL/SQL and also we will do some exercise to get a grip of PL/SQL. So first what is PL/SQL? PL/SQL stands for Procedural Language-Standard Query Language. It is also a case-insensitive programming language. In SQL we are able to execute one statement at a time whereas in PL/SQL we can combine many SQL statements and execute all of them at once. PL/SQL follows a predefined syntax ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:0","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"ER Diagram I made it in a rush, so it might not be as good as it should have been. ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:1","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"Relational Schema Trust me it’s totally correct diagram as because I didn’t messed up anything and let SQL Developer handle it all. What we are going to do? Taking this schema as a base we are going to create few tables. I will just put up all the code to create the tables. ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:2","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"BATCH CREATE TABLE BATCH ( B_ID NUMBER(4, 0) NOT NULL , MAF_DATE DATE NOT NULL , EXP_DATE DATE NOT NULL , B_QTY NUMBER NOT NULL , S_ID NUMBER(3, 0) NOT NULL , CONSTRAINT BATCH_PK PRIMARY KEY ( B_ID ) ); ALTER TABLE BATCH ADD CONSTRAINT BATCH_FK1 FOREIGN KEY ( S_ID ) REFERENCES SUPPLIER ( S_ID ); ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:3","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"CUSTOMER CREATE TABLE CUSTOMER ( C_ID NUMBER(8, 0) NOT NULL , C_NAME VARCHAR2(30 BYTE) NOT NULL , C_PHONE NUMBER NOT NULL , CONSTRAINT CUSTOMER_PK PRIMARY KEY ( C_ID ) ); ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:4","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"EMPOYEE CREATE TABLE EMPLOYEE ( E_ID NUMBER(6, 0) NOT NULL , COUNTER_NO NUMBER NOT NULL , E_PHONE NUMBER(10, 0) NOT NULL , E_NAME VARCHAR2(30 BYTE) NOT NULL , CONSTRAINT EMPLOYEE_PK PRIMARY KEY ( E_ID ) ); ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:5","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"PRODUCT CREATE TABLE PRODUCT ( P_ID NUMBER(5, 0) NOT NULL , P_NAME VARCHAR2(100 BYTE) NOT NULL , P_PRICE NUMBER(9, 2) NOT NULL , B_ID NUMBER(4, 0) NOT NULL , P_QTY NUMBER NOT NULL , CONSTRAINT PRODUCT_PK PRIMARY KEY ( P_ID ) ); ALTER TABLE PRODUCT ADD CONSTRAINT PRODUCT_UK1 UNIQUE ( B_ID ); ALTER TABLE PRODUCT ADD CONSTRAINT PRODUCT_FK1 FOREIGN KEY ( B_ID ) REFERENCES BATCH ( B_ID ); ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:6","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"SALES CREATE TABLE SALES ( C_ID NUMBER(8, 0) NOT NULL , E_ID NUMBER(6, 0) NOT NULL , P_ID NUMBER(5, 0) NOT NULL , PUR_QTY NUMBER NOT NULL , PUR_DATE DATE NOT NULL ); ALTER TABLE SALES ADD CONSTRAINT SALES_FK1 FOREIGN KEY ( C_ID ) REFERENCES CUSTOMER ( C_ID ); ALTER TABLE SALES ADD CONSTRAINT SALES_FK3 FOREIGN KEY ( P_ID ) REFERENCES PRODUCT ( P_ID ); ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:7","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"SUPPLIER CREATE TABLE SUPPLIER ( S_ID NUMBER(3, 0) NOT NULL , S_NAME VARCHAR2(30 BYTE) NOT NULL , S_ADDRESS VARCHAR2(60 BYTE) NOT NULL , S_PHONE NUMBER(10, 0) NOT NULL , S_EMAIL VARCHAR2(30 BYTE) NOT NULL , CONSTRAINT SUPPLIER_PK PRIMARY KEY ( S_ID ) ); ALTER TABLE SUPPLIER ADD CONSTRAINT SUPPLIER_UK1 UNIQUE ( S_PHONE , S_EMAIL ) ","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:8","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":"SUPPLY CREATE TABLE SUPPLY ( S_ID NUMBER(3, 0) NOT NULL , B_ID NUMBER(4, 0) NOT NULL , SUPPLY_DATE DATE NOT NULL , P_ID NUMBER(5, 0) NOT NULL ); ALTER TABLE SUPPLY ADD CONSTRAINT SUPPLY_FK1 FOREIGN KEY ( S_ID ) REFERENCES SUPPLIER ( S_ID ); ALTER TABLE SUPPLY ADD CONSTRAINT SUPPLY_FK2 FOREIGN KEY ( B_ID ) REFERENCES BATCH ( B_ID ); ALTER TABLE SUPPLY ADD CONSTRAINT SUPPLY_FK3 FOREIGN KEY ( P_ID ) REFERENCES PRODUCT ( P_ID ); If you have done Java before then you might know that everything is encapsulated inside so called class. In PL/SQL encapsulated inside so called blocks. Now there are two types of blocks. One is called anonymous block while the other is called named block. As the name suggests the anonymous block does not have any name and thus can’t be used later on( can’t be saved inside a database). While the named block can be referred later on ( as we move on we will come across something like procedure, function and package. Let’s start with an anonymous block. Here’s a syntax for a proper anonymous block: DECLARE Declaration Statements BEGIN Executable statements Exception Exception handling statements END; Here ** Declare ** is used to declare variables. Here’s one example: DECLARE V_NAME VARCHAR2(30); V_PHONE NUMBER(10); V_CONSTANT CONSTANT NUMBER:=0; NOTE: Here we use := to assign a value to a variable. In most of the languages, we use = only. Execution Section starts with BEGIN BEGIN SELECT E_Name, E_ID, E_Phone INTO V_Name, V_ID, V_Phone FROM EMPLOYEE WHERE E_ID =100; DBMS_OUTPUT.PUT_LINE(‘Employee Name ’||V_Name||’ ‘||V_ID ||' '||V_Phone); END; NOTE: To assign the variables value from the table we can use INTO. In the end we can also put Exceptional Handling block. Here’s an example: EXCEPTION WHEN NO_DATA_FOUND THEN DBMS_OUTPUT.PUT_LINE (‘No Employee Found with ’||V_ID); NOTE: Exception Block is optional. You can also leave it. In that case, system will throw an Exception (Which is obviously not user-friendly) Whenever you want to display data to the user using values from the database, you have to use one statement in particular. SET SERVEROUTPUT ON This takes us to a very interesting topic which is totally out of scope in regard to this blog post. Therefore if you really want to know about it contact your DBMS Professor or message me on Twitter or Facebook. Whenever you want to store a value in a variable you first need to provide it a data type.It is best practice to make sure that you have provided the correct data type to correct variable. Let me give you an example. DECLARE V_Name VARCHAR2(15); BEGIN SELECT E_NAME INTO V_Name WHERE E_ID =1005; END; In this code, E_NAME has 30 bytes (VARCHAR2 (30)) but V_Name can only store 15 bytes. Thus we will not get the desired value. For this thing only we use Anchored Datatype (% Type). Here’s the syntax: variable_name typed-attribute%type So if I rewrite my previous code snippet it will be something like this: DECLARE V_Name EMPLOYEE.E_NAME%TYPE; BEGIN SELECT E_NAME INTO V_Name WHERE E_ID =1005; END; IF-THEN-ELSIF Control Statements in PL/SQL Here’s the syntax: IF CONDITION 1 THEN STATEMENT 1; ELSIF CONDITION 2 THEN STATEMENT 2; ELSIF CONDITION 3 THEN STATEMENT 3; ... ELSE STATEMENT N; END IF; Similarly we have LOOP Here’s the syntax: LOOP Statement 1; Statement 2; … Statement 3; END LOOP; CASE in PL/SQL Here’s the syntax: CASE [ expression ] WHEN condition_1 THEN result_1 WHEN condition_2 THEN result_2 ... WHEN condition_n THEN result_n ELSE result END CASE; Here’s an example of how to take a number from the user and determine whether it is even or odd. SET SERVEROUTPUT ON; DECLARE VALUE1 NUMBER:= \u0026NUMBER1; REMAINDER1 NUMBER; BEGIN REMAINDER1 := MOD(VALUE1, 2); CASE REMAINDER1 WHEN 0 THEN DBMS_OUTPUT.PUT_LINE('EVEN NUMBER'); WHEN 1 THEN DBMS_OUTPUT.PUT_LINE('ODD NUMBER'); END CASE; END; Here’s other example in which you can know the day of the week in which you were born. SET SERVEROUTPUT ON; DECLARE V_DATE DATE:= TO_DATE('\u0026BIRTHDAY', 'DD-MON-YYYY'); V_DAY VARCH","date":"2017-07-24","objectID":"/posts/oracle_dbms/oracle-dbms-day-4/:0:9","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 4"],"title":"[Training] DBMS with Oracle Day 4","uri":"/posts/oracle_dbms/oracle-dbms-day-4/"},{"categories":["Database Management","Programming"],"content":" So in this blog basically we are going to cover up the things done by me on the second day of my Oracle DBMS Training. Although I am not going to use the same schema I used during the 2nd day of my training. So without further delay, lets dig in. First thing first. What is a schema? This is something I should have mentioned in the first blog. In general, a schema is a set of tables, sproc(stored procedure) and other objects that make up a whole database. Although in Oracle a user owned all the tables and other objects that together constitute a database and hence in Oracle a user can be considered as a schema. This might sound a bit tough right now but it will totally become clear as you start working with other objects, other than Table. Before starting let me tell you how to write comments in Oracle SQL. For a single line comment we use -- and for a multi line comment we start with /* followed by our comment and the end it with */. Now there are various SQL queries we need to know about. Let’s see the first query. The very first query is CREATE TABLE I might be repeating this but it’s pretty much an essential query among the many other queries. --Syntax: CREATE TABLE \u003cTABLENAME\u003e (COLUMN1 DESCRIPTION, COLUMN2 DESCRIPTION, ....); Here DESCRIPTION contains the data type and if possible the key identifier. Now second query is ALTER TABLE There are plenty of things you can do with ALTER TABLE. Here are few --Syntax to rename a table ALTER TABLE old_name_of_table RENAME TO new_name_of_table; --Syntax to add a column to an existing table ALTER TABLE existing_table_name ADD new_column_name data_type (size); --NOTE: Note here we don't write 'COLUMN' after key word ADD. --Syntax to rename a column of a table. ALTER TABLE table_name RENAME COLUMN old_name_of_column TO new_name_of_column; --Syntax to modify the column definition using ALTER TABLE. ALTER TABLE table_name MODIFY colun_name data_type (size); These are few things we can do with ALTER TABLE command. There are other things too. Now to see the structure of a particular table you can use DESC Syntax: DESC \u003cTABLENAME\u003e Now lets talk about Primary Key Constraint. A Primary Key Constraint basically is a combination of NOT NULL and UNIQUE. If a column is a primary key in that table then each row in that column have some unique value. So how to define a primary key? We can define a primary key at the time of creation of our table. --Example CREATE TABLE PRODUCT ( Product_id NUMBER(5) PRIMARY KEY, Product_name VARCHAR2(30), Product_price NUMBER(5) ); Here Product_id is a primary key. Apart from this, we can also make a primary key as follows: CREATE TABLE PRODUCT ( Product_id NUMBER(3) CONSTRAINT promstr_col1_pid_pk PRIMARY KEY, Product_name VARCHAR2(30), Product_price NUMBER(5) ); Both of the above used queries can be referred as column level definition. Apart from this there is one last method I want yo mention, which is table level definition of a primary key. Here is a small example: CREATE TABLE product_master ( Product_id NUMBER(3), Product_name VARCHAR2(30), Product_price NUMBER(5), CONSTRAINT promstr_col1_pid_pk PRIMARY KEY (product_id) ); **So why do we use the other two methods when we can directly make a column as primary key? ** Basically Primary key is a constraint to a column and and having a different name for a constraint is an efficient way to follow. This makes sure you have different name for column and constraint. And the naming also fits perfectly. You can have more than one primary key too and when you have more than one primary key then it is know as composite key. Here is an example of defining a composite key: CREATE TABLE customer ( cust_id NUMBER(3), cust_name VARCHAR2(3), phone_no NUMBER(10), CONSTRAINT cust_cid_pk PRIMARY KEY ( cust_id, phone_no) ); This was your first constraint definition and that’s why I explained each method separately. From now on I will only focus on the syntax more than examples. Also I am going to attach a document at the end of t","date":"2017-07-20","objectID":"/posts/oracle_dbms/oracle-dbms-day-3/:0:0","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 3"],"title":"[Training] DBMS with Oracle Day 3","uri":"/posts/oracle_dbms/oracle-dbms-day-3/"},{"categories":["Database Management","Programming"],"content":" In this post, I am going to wrap up all the things I learned during my first class on ‘DBMS with Oracle’. The training lecture is given by ‘Rahul Sohal’, CTO \u0026 Resource Management Lead of iandwe.in. So first of all what is database? A database is a collection of information kept an organized way, for the ease of retrieval. Facebook has its own database. Think about yesterday when you logged in your account and like all those posts, share certain stuff, commented on a picture. This all things are stored in a database and when you logged into your account today, all this information are retrieved from the database. **Now next we were taught about what is so called DBMS? ** As because we are Computer Science students we were familiar with the term. We have actually used DBMS like mySQL and Microsoft Access. So a DBMS stands for ‘Database Management System’. Basically it’s a system software for creating as well as managing databases. So it provides a way by which a programmer/user can create, retrieve, update and manipulate the data in different ways. So we are going to learn DBMS with Oracle. As most of us have done mySQL, Rahul sir asked us the difference between mySQL and Oracle DBMS. I have not on Oracle DBMS before but I somewhat knew that Oracle DBMS is used fir more complex queries other the mySQL. In addition, it supports far more join then mySQL which only supports 61 join limit (These things I know as because I have used mySQL for managing database of my own website at some point, and I never have to use any complex queries). Then Rahul sir told us that Oracle is used for ‘Enterprise Business Applications’ in big companies whereas mySQL is used by small companies as it is somewhat easier to use. **Next how data are processed between client and server? ** From client side, we send a request with the help of SQL(Structured Query Language). Then the server returns a tuple on the successful execution of SQL statement. The server is generally hosted on 127.0.0.1:8080. Here 127.0.0.1 is IP and is basically called localhost and 8080 is the port which can be changed during installation of the Oracle DBMS. The data is generally organized as a table containing rows and columns. Here’s a basic example: ROLL NAME 1 Rahul 2 Baka 3 Debakar Here table contains Roll and Name which are attributes and each row contains one data. If we need to display data for roll no. 1 then the query for that is: SELECT * FROM STUDENT WHERE ROLL = 1; CONNECT / AS SYSDBA; To create a user we have to type: CREATE USER \u003cUSERNAME\u003e IDENTIFIED BY \u003cPASSWORD\u003e; Before jumping into creating the table we need to provide some privilege to the newly created user. This thing is quite new to me as in mySQL, we can directly create a table after creating and using one database. In Oracle, we have to assign privilege to the user using SYSDBA. The few common privilege can be assigned like this: GRANT CONNECT TO \u003cUSERNAME\u003e; GRANT RESOURCE, DBA TO \u003cUSERNAME\u003e; GRANT CREATE SESSION TO \u003cUSERNAME\u003e; GRANT UNLIMITED TABLESPACE TO ; After this we can connect to our user like this: CONNECT Enter user-name: your_user_name Enter password: your_password Then we need to give privilege for the basic DML(Data Manipulation Language) commands. This can be done like this: GRANT SELECT, INSERT, UPDATE, DELETE ON \u003cTABLENAME\u003e TO \u003cUSERNAME\u003e But the above command only works after you created a table. This all privilege things are really something to consider in the difference between mySQL and Oracle. Up till now, I get one thing clear, that Oracle DBMS has much greater function than mySQL. CREATE TABLE STUDENT(ROLL NUMBER, NAME VARCHAR2(30), DOB DATE) ; This takes us in a situation where we can talk about something called ‘Data Redundancy’. Rahul Sir told us that in the table we just created we can have as many numbers of the row as we want with same data. Which is nothing but a waste of space. Same data at different rows thus data which is not useful at all is cramming up all the spaces and he","date":"2017-07-18","objectID":"/posts/oracle_dbms/oracle-dbms-day-1/:0:0","tags":["Oracle","DBMS","Database","SQL","Tutorial","Day 1"],"title":"[Training] DBMS with Oracle Day 1","uri":"/posts/oracle_dbms/oracle-dbms-day-1/"},{"categories":["Social Media","Programming","Web Development","Automation"],"content":" First of all what will be this bot doing actually? Ans. This bot will be tweeting any new post on a particular subreddit to your twitter bot account. In my case it will be tweeting any new tweets from /r/anime Can I have a look on the finished product? Ans. Here you go So what do I actually need to create the bot? Ans. Some Basic Programming knowlegde and how to run terminal. You will need to install few software. Get the gist of the code to modify it and make your own bot. So lets begin. ","date":"2017-06-19","objectID":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/:0:0","tags":["Reddit","Twitter","Bot","Automation","API","Node.js","JavaScript","Tutorial","How-to"],"title":"How to build a reddit to twitter bot","uri":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/"},{"categories":["Social Media","Programming","Web Development","Automation"],"content":"Installing node js First of all go to this link and download node js, click the installer depending on your OS. Now run the setup and install it. make sure you did check include PATH. Next open up your terminal and type this code: $ node If your ‘$’ symbol change into ‘\u003e’ symbol then you did everything correct upto this point. ","date":"2017-06-19","objectID":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/:1:0","tags":["Reddit","Twitter","Bot","Automation","API","Node.js","JavaScript","Tutorial","How-to"],"title":"How to build a reddit to twitter bot","uri":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/"},{"categories":["Social Media","Programming","Web Development","Automation"],"content":"Creating node package Now it time to start code. First of all go to the folder where you want to make your bot(create your node package). Then click: $ npm init Now it will ask you for few information. Here’s an example: name: twitter bot version: (0.0.0) 1.0.0 description: My package description goes here. entry point: bot.js test command: git repository: keywords: bot twitter reddit author: Debakar Roy license: (ISC) This will create a json file named package.json. You can edit it as you like in future. [NOTE: What is json? Ans. JSON, or JavaScript Object Notation, is a minimal, readable format for structuring data. It is used primarily to transmit data between a server and web application, as an alternative to XML.] ","date":"2017-06-19","objectID":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/:2:0","tags":["Reddit","Twitter","Bot","Automation","API","Node.js","JavaScript","Tutorial","How-to"],"title":"How to build a reddit to twitter bot","uri":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/"},{"categories":["Social Media","Programming","Web Development","Automation"],"content":"Installing dependecy packages Now its time to install few Node Package Manager. You can head up to npmjs.com to look up for package you need. [NOTE: What is npm? Ans. npm makes it easy for JavaScript developers to share and reuse code, and it makes it easy to update the code that you’re sharing. To be honest they are premade packages which let you use different APIs.] Pakages we need: twit [To talk with twitter.] reddit-snooper [To talk to reddit.] goo.gl [To create short link.] To install pakage use this code: $ npm install twit --save $ npm install reddit-snooper --save $ npm install goo.gl --save It might give you some warning. That’s only because you didn’t fill the package.json files. You dont need to worry about that :). I would Highly recommend you to go through the documentation for each node package to get a basic knowlegde on how to use the packages. ","date":"2017-06-19","objectID":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/:3:0","tags":["Reddit","Twitter","Bot","Automation","API","Node.js","JavaScript","Tutorial","How-to"],"title":"How to build a reddit to twitter bot","uri":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/"},{"categories":["Social Media","Programming","Web Development","Automation"],"content":"Creating bot file Now create a file name bot.js and copy up all this code. For simplicity I have provided few comments to make you understand what are the snippets doing. //Bot start message console.log('Starting bot.'); //nmp modules var Twit = require('twit'); var Snooper = require('reddit-snooper') var googl = require('goo.gl'); //Authenticate twitter var T = new Twit({ consumer_key: 'Consumer key', consumer_secret: 'Consumer Secret', access_token: 'Access Token', access_token_secret: 'Access Token Secret', timeout_ms: 60*1000, // optional HTTP request timeout to apply to all requests. }); //Authenticate reddit snooper = new Snooper({ // credential information is not needed for snooper.watcher username: 'reddit_username', password: 'reddit password', app_id: 'reddit api app id', api_secret: 'reddit api secret', user_agent: 'OPTIONAL user agent for your bot', automatic_retries: true, // automatically handles condition when reddit says 'you are doing this too much' api_requests_per_minuite: 60 // api requests will be spread out in order to play nicely with Reddit }); googl.setKey('Your Google API key'); googl.getKey(); var tweet_text; //reddit new post search snooper.watcher.getPostWatcher('subreddit name') .on('post', function(post) { var shortlink = 'www.reddit.com/' + post.data.permalink; var shortposturl = post.data.url; console.log(shortlink) //Create short url googl.shorten(shortlink) .then(function (shortUrl) { console.log(shortUrl); shortlink = shortUrl; }) .catch(function (err) { console.error(err.message); }); googl.shorten(shortposturl) .then(function (shortUrl) { console.log(shortUrl); shortposturl = shortUrl; }) .catch(function (err) { console.error(err.message); }); //Tweet Generator var tweet_title = 'Title: ' + (post.data.title); if((post.data.title).length\u003e=70){ tweet_title = 'Title: ' + (post.data.title).substring(0, 70) + '...'; } var tweet_url ='\\nUrl: ' + shortposturl; var tweet_discuss = '\\nDiscussion: '+ shortlink; //Tweet tweet_text = tweet_title + tweet_discuss + tweet_url ; //Print tweet to console console.log(tweet_text) tweet_mytext(tweet_text); }) .on('error', console.error) //Function to post the tweet function tweet_mytext(tweet_text){ var tweet = { status: tweet_text } console.log('\\n' + tweet) T.post('statuses/update', tweet, tweeted); function tweeted(err, data, response) { if (err) { console.log(\"Didn't tweeted :(.\"); } else { console.log(\"It worked! Tweeted!!\"); } } } ","date":"2017-06-19","objectID":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/:4:0","tags":["Reddit","Twitter","Bot","Automation","API","Node.js","JavaScript","Tutorial","How-to"],"title":"How to build a reddit to twitter bot","uri":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/"},{"categories":["Social Media","Programming","Web Development","Automation"],"content":"How to get credential needed for setting up bot So there are sevral question you will ask: How to obtain consumer_key, consumer_secret, and other twitter credentials. Ans. For this go to apps.twitter.com. Make sure you are sign in to your twitter bot account. Twitter will not let you create an app until you verified your account. You can use your real phone number or Twilio or any clound communication base SMS service to complete verifying your account. Now click on Create New App. Fill up Name, Description and website.Tick Yes, I have read and agree to the Twitter Developer Agreement. Go to Keys and Access Tokens and you will find your consumer_key and consumer_secret there. To generate access_token and access_token_secret click on create my access token under Token Actions. How to obtain api_secret, app_id, and other reddit credentials. Ans. Steps you need to follow are in the node package description: Create (or log into) a reddit account Navigate to the authorized applications console Select ‘create another app…’ at the bottom Fill in the name, description and click on ‘script’, put in anything for the redirect uri, its not needed and you can change it later if you want to Copy down the ‘secret’ that is your apisecret, the 14 character string by the name of your app is your appid Use these values and your credentials to configure the snooper Now how to obtain the Google API key. Ans. Visit https://console.developers.google.com/cloud-resource-manager and create a new project Search and turn on URL Shortener API Go to Credential on left hand side click on create credential and API key. That’s is your Api key. Pasting all this credentials in bot.js will let you use the bot. Now head to the terminal and type: $ node bot.js ","date":"2017-06-19","objectID":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/:5:0","tags":["Reddit","Twitter","Bot","Automation","API","Node.js","JavaScript","Tutorial","How-to"],"title":"How to build a reddit to twitter bot","uri":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/"},{"categories":["Social Media","Programming","Web Development","Automation"],"content":"Conclusion Congratualation you just made a bot which let you fecth any new post from a particular subreddit and tweet it using you twitter bot account. NOTE: Please Follow twitter, google and reddit rules to ensure not getting ban. This post is only meant for educational purpose. I will not be take any blame if you get offended in any way. Next Tutorial wil be how to host htis bot in Heroku. So that you can let it run even after you turned off your computer. Untill then Stay well and Thank you for reading. ","date":"2017-06-19","objectID":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/:6:0","tags":["Reddit","Twitter","Bot","Automation","API","Node.js","JavaScript","Tutorial","How-to"],"title":"How to build a reddit to twitter bot","uri":"/posts/basics_learning/how-to-create-reddit-to-twitter-bot/"}]